{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé§ LiveTranscriber ‚Äî Real-time Speech Transcription with VAD\n",
    "\n",
    "This notebook implements a **live voice-to-text system** that:\n",
    "- Listens to your microphone input in real-time  \n",
    "- Uses **Silero VAD** (Voice Activity Detection) to detect when you're speaking  \n",
    "- Automatically splits audio into *utterances* based on pauses  \n",
    "- Transcribes each utterance using **Faster-Whisper** (OpenAI Whisper variant)  \n",
    "- Streams the transcribed text chunks as they become available\n",
    "\n",
    "It‚Äôs optimized for local apps (like TUIs or assistants) that need responsive, chunked speech transcription.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Imports and Device Selection\n",
    "\n",
    "We import only the necessary libraries:\n",
    "- `numpy`, `torch`, `pyaudio` for audio and model operations  \n",
    "- `faster_whisper` for the actual speech-to-text model  \n",
    "- `asyncio` for non-blocking background processing  \n",
    "- `logging` for debug output  \n",
    "\n",
    "We also define a small helper to pick the **best available compute device**:\n",
    "- CUDA (GPU)\n",
    "- MPS (Apple Silicon)\n",
    "- or CPU (fallback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/miniconda3/envs/tui_writer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import logging\n",
    "import asyncio\n",
    "from typing import Optional, Callable\n",
    "from queue import Queue\n",
    "\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "def get_device(force_cpu: bool = False) -> str:\n",
    "    \"\"\"Pick best available device.\"\"\"\n",
    "    if force_cpu:\n",
    "        return \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def load_silero_vad():\n",
    "    \"\"\"Load Silero VAD model from torch hub.\"\"\"\n",
    "    try:\n",
    "        model, _utils = torch.hub.load(\n",
    "            repo_or_dir='snakers4/silero-vad',\n",
    "            model='silero_vad',\n",
    "            force_reload=False,\n",
    "            onnx=False\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to load Silero VAD: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó£Ô∏è Silero VAD (Voice Activity Detection)\n",
    "\n",
    "This small helper loads the **Silero VAD** model from `torch.hub`.\n",
    "\n",
    "Silero VAD is a lightweight neural model that outputs the **probability of speech** (0‚Äì1).  \n",
    "We‚Äôll use it to decide when the user is talking or has paused ‚Äî so we can send only meaningful speech chunks to Whisper.\n",
    "\n",
    "If Silero fails to load (e.g., offline), we just return `None` and handle it later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è LiveTranscriber ‚Äî Real-time Speech-to-Text\n",
    "\n",
    "The `LiveTranscriber` class captures live audio from your microphone, detects when you‚Äôre speaking using **Silero VAD**, and transcribes each spoken sentence using **Faster-Whisper** once you pause.\n",
    "\n",
    "It runs asynchronously, making it ideal for real-time interfaces like TUIs or assistants.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Parameters\n",
    "- **`model_id`** ‚Äì Whisper model to use (e.g. `\"tiny\"`, `\"base\"`, `\"small\"`).  \n",
    "- **`language`** ‚Äì Language code for transcription (default `\"en\"`).  \n",
    "- **`force_cpu`** ‚Äì Force CPU usage even if GPU is available.  \n",
    "- **`on_transcript`** ‚Äì Callback called with each transcribed text chunk.  \n",
    "- **`vad_threshold`** ‚Äì Silero confidence threshold (0.0‚Äì1.0, higher = stricter).  \n",
    "- **`min_speech_duration_ms`** ‚Äì Minimum length of speech to count as valid.  \n",
    "- **`min_silence_duration_ms`** ‚Äì How long silence must last before starting transcription.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Main Methods\n",
    "- **`start()`** ‚Äî Begins microphone capture and transcription loop (async).  \n",
    "- **`stop()`** ‚Äî Gracefully stops audio processing.  \n",
    "- **`process_audio()`** ‚Äî Runs continuously, detecting speech/silence and triggering transcription.  \n",
    "- **`_transcribe_chunk()`** ‚Äî Uses Whisper to transcribe one full utterance.  \n",
    "- **`_detect_speech_silero()`** ‚Äî Returns `True` if Silero VAD detects speech in the current chunk.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Summary\n",
    "- Audio is streamed in 32 ms chunks (512 samples at 16 kHz).  \n",
    "- Each chunk is passed to Silero VAD ‚Üí speech or silence.  \n",
    "- When silence lasts long enough, the buffered audio is sent to Whisper.  \n",
    "- The result is sent to your `on_transcript` callback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LiveTranscriber:\n",
    "    \"\"\"\n",
    "    Improved LiveTranscriber:\n",
    "      - uses asyncio.Queue for clean async consumption\n",
    "      - pushes audio from audio thread with loop.call_soon_threadsafe\n",
    "      - collects chunks in a list (no repeated np.append)\n",
    "      - does NOT append silence chunks into the sent buffer\n",
    "      - dispatches transcription to a thread and safely calls callbacks\n",
    "      - provides stop() for clean shutdown\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = \"openai/whisper-base\",\n",
    "        language: str = \"en\",\n",
    "        force_cpu: bool = False,\n",
    "        on_transcript: Optional[Callable[[str], None]] = None,\n",
    "        vad_threshold: float = 0.5,\n",
    "        min_speech_duration_ms: int = 250,\n",
    "        min_silence_duration_ms: int = 500,\n",
    "    ):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.on_transcript = on_transcript\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.language = language\n",
    "\n",
    "        self.sample_rate = 16000\n",
    "\n",
    "        # ASR model\n",
    "        self.device = get_device(force_cpu=force_cpu)\n",
    "        self.transcribe_model = WhisperModel(\n",
    "            self.model_id,\n",
    "            device=self.device,\n",
    "            compute_type=\"int8\" if self.device == \"cpu\" else \"float16\",\n",
    "        )\n",
    "\n",
    "        # VAD\n",
    "        self.vad_threshold = vad_threshold\n",
    "        self.silero_model = load_silero_vad()\n",
    "        if self.silero_model is None:\n",
    "            raise RuntimeError(\"Silero VAD failed to load. Cannot continue.\")\n",
    "\n",
    "        # thresholds in samples\n",
    "        self.min_speech_samples = int(self.sample_rate * min_speech_duration_ms / 1000)\n",
    "        self.min_silence_samples = int(self.sample_rate * min_silence_duration_ms / 1000)\n",
    "\n",
    "        # async queue and runtime vars (set in start())\n",
    "        self.async_queue: Optional[asyncio.Queue] = None\n",
    "        self.loop: Optional[asyncio.AbstractEventLoop] = None\n",
    "\n",
    "        # state used by process_audio\n",
    "        self.is_running = False\n",
    "        self.is_speech_active = False\n",
    "        self.speech_chunks: list[np.ndarray] = []  # collect speech chunks here (no np.append)\n",
    "        self.speech_samples = 0  # total samples currently in speech_chunks\n",
    "        self.silence_counter = 0  # in samples\n",
    "\n",
    "        self._pyaudio = None\n",
    "        self._stream = None\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"LiveTranscriber init (model={model_id}, device={self.device}, rate={self.sample_rate})\"\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # VAD & transcription helpers\n",
    "    # -------------------------\n",
    "    def _detect_speech_silero(self, audio_chunk: np.ndarray) -> bool:\n",
    "        \"\"\"Return True if Silero considers this chunk speech.\"\"\"\n",
    "        try:\n",
    "            audio_tensor = torch.from_numpy(audio_chunk).float()\n",
    "            prob = self.silero_model(audio_tensor, self.sample_rate).item()\n",
    "            return prob > self.vad_threshold\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Silero VAD error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _transcribe_chunk(self, audio_data: np.ndarray) -> str:\n",
    "        \"\"\"Blocking transcription call. Run in thread via asyncio.to_thread.\"\"\"\n",
    "        segments, _info = self.transcribe_model.transcribe(\n",
    "            audio_data,\n",
    "            language=self.language,\n",
    "            beam_size=1,\n",
    "            condition_on_previous_text=False,\n",
    "            vad_filter=True,\n",
    "            vad_parameters=dict(\n",
    "                threshold=0.4,\n",
    "                min_speech_duration_ms=int(self.min_speech_samples * 1000 / self.sample_rate),\n",
    "                max_speech_duration_s=float(\"inf\"),\n",
    "                min_silence_duration_ms=200,\n",
    "            ),\n",
    "        )\n",
    "        return \" \".join(s.text.strip() for s in segments).strip()\n",
    "\n",
    "    async def _run_transcribe_and_callback(self, buffer_copy: np.ndarray) -> None:\n",
    "        \"\"\"Run transcription in a worker thread and call user callback safely.\"\"\"\n",
    "        try:\n",
    "            text = await asyncio.to_thread(self._transcribe_chunk, buffer_copy)\n",
    "        except Exception as e:\n",
    "            self.logger.exception(f\"Transcription failed: {e}\")\n",
    "            return\n",
    "\n",
    "        if not text:\n",
    "            return\n",
    "\n",
    "        if not self.on_transcript:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            if asyncio.iscoroutinefunction(self.on_transcript):\n",
    "                await self.on_transcript(text)\n",
    "            else:\n",
    "                await asyncio.to_thread(self.on_transcript, text)\n",
    "        except Exception as e:\n",
    "            self.logger.exception(f\"on_transcript callback failed: {e}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # PyAudio callback -> async queue\n",
    "    # -------------------------\n",
    "    def audio_callback(self, in_data, frame_count, time_info, status):\n",
    "        \"\"\"Runs in PyAudio native thread. Put chunk into asyncio.Queue via loop.call_soon_threadsafe.\"\"\"\n",
    "        if status:\n",
    "            self.logger.debug(f\"Audio callback status: {status}\")\n",
    "        audio = np.frombuffer(in_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "        # the audio stream starts only after start() sets self.loop and self.async_queue\n",
    "        if self.loop and self.async_queue:\n",
    "            # safe: put_nowait from the audio thread via call_soon_threadsafe\n",
    "            self.loop.call_soon_threadsafe(self.async_queue.put_nowait, audio)\n",
    "        else:\n",
    "            # fallback: drop audio if not ready\n",
    "            self.logger.debug(\"Dropping audio: loop or async_queue not ready\")\n",
    "\n",
    "        return in_data, pyaudio.paContinue\n",
    "\n",
    "    # -------------------------\n",
    "    # Main async processing loop\n",
    "    # -------------------------\n",
    "    async def process_audio(self):\n",
    "        \"\"\"Consume audio chunks from self.async_queue and run VAD/chunking with minimal nesting.\"\"\"\n",
    "        assert self.async_queue is not None\n",
    "\n",
    "        while self.is_running:\n",
    "            # await next chunk (clean, no busy-sleep)\n",
    "            try:\n",
    "                chunk: np.ndarray = await self.async_queue.get()\n",
    "            except asyncio.CancelledError:\n",
    "                break\n",
    "\n",
    "            is_speech = self._detect_speech_silero(chunk)\n",
    "\n",
    "            if is_speech:\n",
    "                # start or extend a speech buffer (we only append speech chunks)\n",
    "                if not self.is_speech_active:\n",
    "                    # You just STARTED talking\n",
    "                    self.is_speech_active = True\n",
    "                    self.speech_chunks = [chunk.copy()]     # Start new buffer\n",
    "                    self.speech_samples = len(chunk)        # Count samples\n",
    "                    self.silence_counter = 0                # Reset silence timer\n",
    "                else:\n",
    "                    # You're STILL talking\n",
    "                    self.speech_chunks.append(chunk)        # Add to buffer\n",
    "                    self.speech_samples += len(chunk)       # Count more samples\n",
    "                    self.silence_counter = 0                # Reset silence timer\n",
    "                continue    # Skip to next chunk\n",
    "\n",
    "            # chunk is silence\n",
    "            if not self.is_speech_active:\n",
    "                # You're still quiet, nothing to do\n",
    "                continue    # Skip to next chunk\n",
    "\n",
    "            # we were in speech; got a silence chunk -> count it\n",
    "            self.silence_counter += len(chunk)\n",
    "\n",
    "            if self.silence_counter < self.min_silence_samples:\n",
    "                # not enough silence yet to finalize\n",
    "                continue\n",
    "\n",
    "            # enough silence observed -> finalize this speech segment\n",
    "            if self.speech_samples >= self.min_speech_samples:\n",
    "\n",
    "                # 1. Combine all speech chunks into one audio buffer\n",
    "                if len(self.speech_chunks) > 1:\n",
    "                    buffer_copy = np.concatenate(self.speech_chunks)  # Multiple chunks\n",
    "                else:\n",
    "                    buffer_copy = self.speech_chunks[0].copy()        # Single chunk\n",
    "                \n",
    "                # 2. IMMEDIATELY reset state (so we can capture new speech)\n",
    "                self.is_speech_active = False\n",
    "                self.speech_chunks = []\n",
    "                self.speech_samples = 0\n",
    "                self.silence_counter = 0\n",
    "\n",
    "                # 3. Send to Whisper (in background, don't wait for it)\n",
    "                asyncio.create_task(self._run_transcribe_and_callback(buffer_copy))\n",
    "            else:\n",
    "                # Speech was too short (< 250ms), just ignore it\n",
    "                self.is_speech_active = False\n",
    "                self.speech_chunks = []\n",
    "                self.speech_samples = 0\n",
    "                self.silence_counter = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # Start / stop helpers\n",
    "    # -------------------------\n",
    "    async def start(self):\n",
    "        \"\"\"Start audio stream and processing loop. Returns when process_audio finishes (stop() called).\"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        self.loop = asyncio.get_running_loop()\n",
    "        self.async_queue = asyncio.Queue()\n",
    "        self.is_running = True\n",
    "\n",
    "        # start pyaudio stream\n",
    "        self._pyaudio = pyaudio.PyAudio()\n",
    "        self._stream = self._pyaudio.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=1,\n",
    "            rate=self.sample_rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=512,\n",
    "            stream_callback=self.audio_callback,\n",
    "        )\n",
    "        self._stream.start_stream()\n",
    "\n",
    "        try:\n",
    "            await self.process_audio()\n",
    "        finally:\n",
    "            # cleanup\n",
    "            try:\n",
    "                self._stream.stop_stream()\n",
    "                self._stream.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                self._pyaudio.terminate()\n",
    "            except Exception:\n",
    "                pass\n",
    "            self.is_running = False\n",
    "            self.loop = None\n",
    "            self.async_queue = None\n",
    "\n",
    "    async def stop(self, transcribe_remaining: bool = True):\n",
    "        \"\"\"Signal the processing loop to stop and optionally transcribe remaining speech.\"\"\"\n",
    "        self.is_running = False\n",
    "        \n",
    "        # If we have buffered speech and want to transcribe it\n",
    "        if transcribe_remaining and self.is_speech_active and len(self.speech_chunks) > 0:\n",
    "            # Combine speech chunks\n",
    "            if len(self.speech_chunks) > 1:\n",
    "                buffer_copy = np.concatenate(self.speech_chunks)\n",
    "            else:\n",
    "                buffer_copy = self.speech_chunks[0].copy()\n",
    "            \n",
    "            # Clear state\n",
    "            self.is_speech_active = False\n",
    "            self.speech_chunks = []\n",
    "            self.speech_samples = 0\n",
    "            self.silence_counter = 0\n",
    "            \n",
    "            # Transcribe immediately (wait for completion)\n",
    "            await self._run_transcribe_and_callback(buffer_copy)\n",
    "        \n",
    "        # Wait for process_audio to exit\n",
    "        await asyncio.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Live Transcription Test\n",
    "\n",
    "This test starts a 10-second live recording session using the `LiveTranscriber`.  \n",
    "Speak naturally in short sentences ‚Äî each pause will automatically trigger a transcription.  \n",
    "Each transcribed chunk is printed as soon as it‚Äôs ready, and all results are shown at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Speak in short sentences; pauses will trigger transcription.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jens/.cache/torch/hub/snakers4_silero-vad_master\n",
      "ALSA lib pcm_dsnoop.c:567:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRANSCRIBED] Okay, can you hear me?\n",
      "\n",
      "[TRANSCRIBED] That's nice.\n",
      "\n",
      "[TRANSCRIBED] What if now?\n",
      "\n",
      "[TRANSCRIBED] and now\n",
      "\n",
      "üìù Full transcript:\n",
      "1. Okay, can you hear me?\n",
      "2. That's nice.\n",
      "3. What if now?\n",
      "4. and now\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import asyncio\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "def handle_transcript_chunk(text: str):\n",
    "    \"\"\"Callback called whenever a transcription chunk is ready.\"\"\"\n",
    "    if text.strip():\n",
    "        print(f\"\\n[TRANSCRIBED] {text}\")\n",
    "        all_chunks.append(text)\n",
    "\n",
    "async def test_live_transcription(duration_seconds: int = 10):\n",
    "    all_chunks.clear()\n",
    "    print(\"üé§ Speak in short sentences; pauses will trigger transcription.\")\n",
    "    transcriber = LiveTranscriber(\n",
    "        model_id=\"base\",          # keep whatever model id you use\n",
    "        language=\"en\",\n",
    "        on_transcript=handle_transcript_chunk,\n",
    "        vad_threshold=0.5,\n",
    "        min_speech_duration_ms=250,\n",
    "        min_silence_duration_ms=500,\n",
    "    )\n",
    "\n",
    "    # start the transcriber in the background\n",
    "    start_task = asyncio.create_task(transcriber.start())\n",
    "\n",
    "    try:\n",
    "        # run for given duration (you can interrupt with Ctrl+C)\n",
    "        await asyncio.sleep(duration_seconds)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user.\")\n",
    "    finally:\n",
    "        # ask the transcriber to stop and wait for it to finish\n",
    "        await transcriber.stop()\n",
    "\n",
    "        # wait for the start() task to exit cleanly\n",
    "        try:\n",
    "            await start_task\n",
    "        except Exception as e:\n",
    "            # if any error bubbled up from start/process_audio, show it\n",
    "            print(f\"Transcriber task ended with exception: {e}\")\n",
    "\n",
    "    print(\"\\nüìù Full transcript:\")\n",
    "    for i, t in enumerate(all_chunks, 1):\n",
    "        print(f\"{i}. {t}\")\n",
    "\n",
    "# run it\n",
    "await test_live_transcription(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tui_writer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
