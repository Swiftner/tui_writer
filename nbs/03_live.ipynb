{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: transcription.html\n",
    "title: Transcription\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Transcription\n",
    "\n",
    "This module implements a real-time speech-to-text system that: <br>\n",
    "- Listens to microphone input continuously <br>\n",
    "- Uses **Silero VAD** (Voice Activity Detection) to detect when you are speaking <br>\n",
    "- Automatically segments audio into utterances based on natural pauses <br>\n",
    "- Transcribes each utterance using **Faster-Whisper** (optimized Whisper implementation) <br>\n",
    "- Streams transcribed text chunks as they become available <br>\n",
    "\n",
    "The system is optimized for interactive applications that need responsive, chunked speech transcription with minimal latency.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "The implementation uses a multi-threaded, asynchronous architecture:\n",
    "\n",
    "1. **PyAudio Thread**: Captures raw audio from the microphone <br>\n",
    "2. **AsyncIO Queue**: Thread-safe bridge between audio callback and async processing <br>\n",
    "3. **Processing Loop**: Analyzes audio with VAD and triggers transcription <br>\n",
    "4. **Worker Threads**: Runs Whisper transcription without blocking the main loop <br>\n",
    "\n",
    "This design ensures audio capture never blocks or drops frames while maintaining responsive transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Device Selection\n",
    "\n",
    "The module imports only essential dependencies: <br>\n",
    "- **numpy**: Audio data manipulation and buffer management <br>\n",
    "- **torch**: Required for Silero VAD model <br>\n",
    "- **pyaudio**: Cross-platform audio I/O <br>\n",
    "- **faster_whisper**: Optimized Whisper implementation with CTranslate2 backend <br>\n",
    "- **asyncio**: Non-blocking audio processing <br>\n",
    "- **logging**: Debug and status output <br>\n",
    "\n",
    "### Device Selection Strategy\n",
    "\n",
    "The `get_device()` helper automatically selects the best available compute device:\n",
    "\n",
    "1. **CPU (forced)**: If `force_cpu=True` <br>\n",
    "2. **CUDA**: NVIDIA GPU acceleration (if available) <br>\n",
    "3. **MPS**: Apple Silicon Metal Performance Shaders (if available) <br>\n",
    "4. **CPU (fallback)**: Default fallback <br>\n",
    "\n",
    "For Whisper models, device selection impacts speed significantly: <br>\n",
    "- CPU: Slower but works everywhere <br>\n",
    "- CUDA: 5-10x faster on compatible NVIDIA GPUs <br>\n",
    "- MPS: 3-5x faster on M1/M2/M3 Macs <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| include: false\n",
    "import logging\n",
    "import asyncio\n",
    "from typing import Optional, Callable\n",
    "from queue import Queue\n",
    "\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "def get_device(force_cpu: bool = False) -> str:\n",
    "    \"\"\"Pick best available device.\"\"\"\n",
    "    if force_cpu:\n",
    "        return \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def load_silero_vad():\n",
    "    \"\"\"Load Silero VAD model from torch hub.\"\"\"\n",
    "    try:\n",
    "        model, _utils = torch.hub.load(\n",
    "            repo_or_dir='snakers4/silero-vad',\n",
    "            model='silero_vad',\n",
    "            force_reload=False,\n",
    "            onnx=False\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to load Silero VAD: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silero VAD - Voice Activity Detection\n",
    "\n",
    "The `load_silero_vad()` function loads the **Silero VAD** model from PyTorch Hub.\n",
    "\n",
    "### What is VAD?\n",
    "\n",
    "Voice Activity Detection is a technique for identifying segments of audio that contain human speech. Silero VAD is a lightweight neural network that outputs a probability score (0.0 to 1.0) indicating the likelihood that a given audio chunk contains speech.\n",
    "\n",
    "### Why VAD Matters\n",
    "\n",
    "Without VAD, a transcription system would: <br>\n",
    "- Send every audio chunk to Whisper (expensive and slow) <br>\n",
    "- Transcribe silence and background noise <br>\n",
    "- Have difficulty segmenting continuous speech into utterances <br>\n",
    "\n",
    "With VAD, the system: <br>\n",
    "- Only transcribes when speech is detected <br>\n",
    "- Automatically segments based on natural pauses <br>\n",
    "- Reduces API costs and improves responsiveness <br>\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "The Silero VAD model: <br>\n",
    "- Accepts 16 kHz audio tensors <br>\n",
    "- Returns a single float between 0.0 and 1.0 <br>\n",
    "- Runs extremely fast (< 1ms per chunk on CPU) <br>\n",
    "- Requires no internet connection after initial download <br>\n",
    "\n",
    "If loading fails (offline, network issues), the function returns `None` and the transcriber raises a runtime error since VAD is essential for operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiveTranscriber Class\n",
    "\n",
    "The `LiveTranscriber` class captures live audio from your microphone, detects when you are speaking using **Silero VAD**, and transcribes each spoken sentence using **Faster-Whisper** once you pause.\n",
    "\n",
    "It runs asynchronously, making it ideal for real-time interfaces like TUIs or voice assistants.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **model_id** (`str`): Whisper model to use <br>\n",
    "  - Options: `\"tiny\"`, `\"base\"`, `\"small\"`, `\"medium\"`, `\"large-v3\"` <br>\n",
    "  - Trade-off: Speed vs. accuracy (tiny is fastest, large is most accurate) <br>\n",
    "  - Default: `\"openai/whisper-base\"` <br>\n",
    "\n",
    "- **language** (`str`): ISO language code for transcription <br>\n",
    "  - Default: `\"en\"` (English) <br>\n",
    "  - Supports 40+ languages (see Whisper documentation) <br>\n",
    "\n",
    "- **force_cpu** (`bool`): Force CPU usage even if GPU is available <br>\n",
    "  - Useful for systems where GPU is reserved for other tasks <br>\n",
    "  - Default: `False` <br>\n",
    "\n",
    "- **on_transcript** (`Callable`): Callback function called with each transcribed text chunk <br>\n",
    "  - Signature: `def callback(text: str) -> None` or `async def callback(text: str) -> None` <br>\n",
    "  - Called once per utterance (speech segment followed by silence) <br>\n",
    "\n",
    "- **vad_threshold** (`float`): Silero confidence threshold (0.0‚Äì1.0) <br>\n",
    "  - Higher values require more confident speech detection <br>\n",
    "  - Lower values are more sensitive but may catch background noise <br>\n",
    "  - Default: `0.5` (balanced) <br>\n",
    "\n",
    "- **min_speech_duration_ms** (`int`): Minimum length of speech to count as valid <br>\n",
    "  - Filters out very brief sounds that might be noise <br>\n",
    "  - Default: `250` ms <br>\n",
    "\n",
    "- **min_silence_duration_ms** (`int`): How long silence must last before triggering transcription <br>\n",
    "  - Controls utterance segmentation <br>\n",
    "  - Lower values create more frequent, shorter transcriptions <br>\n",
    "  - Higher values wait longer before processing <br>\n",
    "  - Default: `500` ms <br>\n",
    "\n",
    "### Key Methods\n",
    "\n",
    "#### start() - Begin Transcription\n",
    "Starts the microphone capture and transcription loop. Runs asynchronously until `stop()` is called.\n",
    "\n",
    "```python\n",
    "await transcriber.start()\n",
    "```\n",
    "\n",
    "#### stop(transcribe_remaining=True) - Stop Transcription\n",
    "Gracefully stops audio processing. If `transcribe_remaining=True`, processes any buffered speech before exiting.\n",
    "\n",
    "```python\n",
    "await transcriber.stop()\n",
    "```\n",
    "\n",
    "#### process_audio() - Internal Processing Loop\n",
    "Continuously consumes audio chunks from the queue, runs VAD, and triggers transcription. Automatically called by `start()`.\n",
    "\n",
    "#### _transcribe_chunk(audio_data) - Transcription\n",
    "Uses Whisper to transcribe one complete utterance. Runs in a worker thread to avoid blocking.\n",
    "\n",
    "#### _detect_speech_silero(audio_chunk) - VAD Analysis\n",
    "Returns `True` if Silero VAD detects speech in the current chunk.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Audio Capture**: PyAudio streams 512-sample chunks (32 ms at 16 kHz) from the microphone <br>\n",
    "2. **Thread Safety**: Audio callback pushes chunks to an AsyncIO queue via `call_soon_threadsafe` <br>\n",
    "3. **VAD Processing**: Each chunk is analyzed by Silero VAD <br>\n",
    "4. **Speech Buffering**: Speech chunks are accumulated in a list <br>\n",
    "5. **Silence Detection**: When silence persists for `min_silence_duration_ms`, buffered speech is sent to Whisper <br>\n",
    "6. **Transcription**: Whisper runs in a worker thread and calls `on_transcript` with the result <br>\n",
    "7. **State Reset**: Buffer is cleared and the system is ready for the next utterance <br>\n",
    "\n",
    "### State Machine\n",
    "\n",
    "```\n",
    "IDLE (waiting for speech)\n",
    "  ‚Üì (speech detected)\n",
    "RECORDING (buffering speech chunks)\n",
    "  ‚Üì (silence detected for min_silence_duration_ms)\n",
    "TRANSCRIBING (Whisper processes buffered audio)\n",
    "  ‚Üì (transcription complete)\n",
    "IDLE (ready for next utterance)\n",
    "```\n",
    "\n",
    "### Thread Safety\n",
    "\n",
    "The implementation carefully manages threading: <br>\n",
    "- **PyAudio thread**: Runs the audio callback <br>\n",
    "- **AsyncIO thread**: Runs the processing loop <br>\n",
    "- **Worker threads**: Run Whisper transcription <br>\n",
    "\n",
    "Communication between threads uses `asyncio.Queue` and `loop.call_soon_threadsafe()` to ensure thread safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| include: false\n",
    "\n",
    "class LiveTranscriber:\n",
    "    \"\"\"\n",
    "    Improved LiveTranscriber:\n",
    "      - uses asyncio.Queue for clean async consumption\n",
    "      - pushes audio from audio thread with loop.call_soon_threadsafe\n",
    "      - collects chunks in a list (no repeated np.append)\n",
    "      - does NOT append silence chunks into the sent buffer\n",
    "      - dispatches transcription to a thread and safely calls callbacks\n",
    "      - provides stop() for clean shutdown\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = \"openai/whisper-base\",\n",
    "        language: str = \"en\",\n",
    "        force_cpu: bool = False,\n",
    "        on_transcript: Optional[Callable[[str], None]] = None,\n",
    "        vad_threshold: float = 0.5,\n",
    "        min_speech_duration_ms: int = 250,\n",
    "        min_silence_duration_ms: int = 500,\n",
    "    ):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.on_transcript = on_transcript\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.language = language\n",
    "\n",
    "        self.sample_rate = 16000\n",
    "\n",
    "        # ASR model\n",
    "        self.device = get_device(force_cpu=force_cpu)\n",
    "        self.transcribe_model = WhisperModel(\n",
    "            self.model_id,\n",
    "            device=self.device,\n",
    "            compute_type=\"int8\" if self.device == \"cpu\" else \"float16\",\n",
    "        )\n",
    "\n",
    "        # VAD\n",
    "        self.vad_threshold = vad_threshold\n",
    "        self.silero_model = load_silero_vad()\n",
    "        if self.silero_model is None:\n",
    "            raise RuntimeError(\"Silero VAD failed to load. Cannot continue.\")\n",
    "\n",
    "        # thresholds in samples\n",
    "        self.min_speech_samples = int(self.sample_rate * min_speech_duration_ms / 1000)\n",
    "        self.min_silence_samples = int(self.sample_rate * min_silence_duration_ms / 1000)\n",
    "\n",
    "        # async queue and runtime vars (set in start())\n",
    "        self.async_queue: Optional[asyncio.Queue] = None\n",
    "        self.loop: Optional[asyncio.AbstractEventLoop] = None\n",
    "\n",
    "        # state used by process_audio\n",
    "        self.is_running = False\n",
    "        self.is_speech_active = False\n",
    "        self.speech_chunks: list[np.ndarray] = []  # collect speech chunks here (no np.append)\n",
    "        self.speech_samples = 0  # total samples currently in speech_chunks\n",
    "        self.silence_counter = 0  # in samples\n",
    "\n",
    "        self._pyaudio = None\n",
    "        self._stream = None\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"LiveTranscriber init (model={model_id}, device={self.device}, rate={self.sample_rate})\"\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # VAD & transcription helpers\n",
    "    # -------------------------\n",
    "    def _detect_speech_silero(self, audio_chunk: np.ndarray) -> bool:\n",
    "        \"\"\"Return True if Silero considers this chunk speech.\"\"\"\n",
    "        try:\n",
    "            audio_tensor = torch.from_numpy(audio_chunk).float()\n",
    "            prob = self.silero_model(audio_tensor, self.sample_rate).item()\n",
    "            return prob > self.vad_threshold\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Silero VAD error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _transcribe_chunk(self, audio_data: np.ndarray) -> str:\n",
    "        \"\"\"Blocking transcription call. Run in thread via asyncio.to_thread.\"\"\"\n",
    "        segments, _info = self.transcribe_model.transcribe(\n",
    "            audio_data,\n",
    "            language=self.language,\n",
    "            beam_size=1,\n",
    "            condition_on_previous_text=False,\n",
    "            vad_filter=False,\n",
    "        )\n",
    "        return \" \".join(s.text.strip() for s in segments).strip()\n",
    "\n",
    "    async def _run_transcribe_and_callback(self, buffer_copy: np.ndarray) -> None:\n",
    "        \"\"\"Run transcription in a worker thread and call user callback safely.\"\"\"\n",
    "        try:\n",
    "            text = await asyncio.to_thread(self._transcribe_chunk, buffer_copy)\n",
    "        except Exception as e:\n",
    "            self.logger.exception(f\"Transcription failed: {e}\")\n",
    "            return\n",
    "\n",
    "        if not text:\n",
    "            return\n",
    "\n",
    "        if not self.on_transcript:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            if asyncio.iscoroutinefunction(self.on_transcript):\n",
    "                await self.on_transcript(text)\n",
    "            else:\n",
    "                await asyncio.to_thread(self.on_transcript, text)\n",
    "        except Exception as e:\n",
    "            self.logger.exception(f\"on_transcript callback failed: {e}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # PyAudio callback -> async queue\n",
    "    # -------------------------\n",
    "    def audio_callback(self, in_data, frame_count, time_info, status):\n",
    "        \"\"\"Runs in PyAudio native thread. Put chunk into asyncio.Queue via loop.call_soon_threadsafe.\"\"\"\n",
    "        if status:\n",
    "            self.logger.debug(f\"Audio callback status: {status}\")\n",
    "        audio = np.frombuffer(in_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "        # the audio stream starts only after start() sets self.loop and self.async_queue\n",
    "        if self.loop and self.async_queue:\n",
    "            # safe: put_nowait from the audio thread via call_soon_threadsafe\n",
    "            self.loop.call_soon_threadsafe(self.async_queue.put_nowait, audio)\n",
    "        else:\n",
    "            # fallback: drop audio if not ready\n",
    "            self.logger.debug(\"Dropping audio: loop or async_queue not ready\")\n",
    "\n",
    "        return in_data, pyaudio.paContinue\n",
    "\n",
    "    # -------------------------\n",
    "    # Main async processing loop\n",
    "    # -------------------------\n",
    "    async def process_audio(self):\n",
    "        \"\"\"Consume audio chunks from self.async_queue and run VAD/chunking with minimal nesting.\"\"\"\n",
    "        assert self.async_queue is not None\n",
    "\n",
    "        while self.is_running:\n",
    "            # await next chunk (clean, no busy-sleep)\n",
    "            try:\n",
    "                chunk: np.ndarray = await self.async_queue.get()\n",
    "            except asyncio.CancelledError:\n",
    "                break\n",
    "\n",
    "            is_speech = self._detect_speech_silero(chunk)\n",
    "\n",
    "            if is_speech:\n",
    "                # start or extend a speech buffer (we only append speech chunks)\n",
    "                if not self.is_speech_active:\n",
    "                    # You just STARTED talking\n",
    "                    self.is_speech_active = True\n",
    "                    self.speech_chunks = [chunk.copy()]     # Start new buffer\n",
    "                    self.speech_samples = len(chunk)        # Count samples\n",
    "                    self.silence_counter = 0                # Reset silence timer\n",
    "                else:\n",
    "                    # You're STILL talking\n",
    "                    self.speech_chunks.append(chunk)        # Add to buffer\n",
    "                    self.speech_samples += len(chunk)       # Count more samples\n",
    "                    self.silence_counter = 0                # Reset silence timer\n",
    "                continue    # Skip to next chunk\n",
    "\n",
    "            # chunk is silence\n",
    "            if not self.is_speech_active:\n",
    "                # You're still quiet, nothing to do\n",
    "                continue    # Skip to next chunk\n",
    "\n",
    "            # we were in speech; got a silence chunk -> count it\n",
    "            self.silence_counter += len(chunk)\n",
    "\n",
    "            if self.silence_counter < self.min_silence_samples:\n",
    "                # not enough silence yet to finalize\n",
    "                continue\n",
    "\n",
    "            # enough silence observed -> finalize this speech segment\n",
    "            if self.speech_samples >= self.min_speech_samples:\n",
    "\n",
    "                # 1. Combine all speech chunks into one audio buffer\n",
    "                if len(self.speech_chunks) > 1:\n",
    "                    buffer_copy = np.concatenate(self.speech_chunks)  # Multiple chunks\n",
    "                else:\n",
    "                    buffer_copy = self.speech_chunks[0].copy()        # Single chunk\n",
    "                \n",
    "                # 2. IMMEDIATELY reset state (so we can capture new speech)\n",
    "                self.is_speech_active = False\n",
    "                self.speech_chunks = []\n",
    "                self.speech_samples = 0\n",
    "                self.silence_counter = 0\n",
    "\n",
    "                # 3. Send to Whisper (in background, don't wait for it)\n",
    "                asyncio.create_task(self._run_transcribe_and_callback(buffer_copy))\n",
    "            else:\n",
    "                # Speech was too short (< 250ms), just ignore it\n",
    "                self.is_speech_active = False\n",
    "                self.speech_chunks = []\n",
    "                self.speech_samples = 0\n",
    "                self.silence_counter = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # Start / stop helpers\n",
    "    # -------------------------\n",
    "    async def start(self):\n",
    "        \"\"\"Start audio stream and processing loop. Returns when process_audio finishes (stop() called).\"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        self.loop = asyncio.get_running_loop()\n",
    "        self.async_queue = asyncio.Queue()\n",
    "        self.is_running = True\n",
    "\n",
    "        # start pyaudio stream\n",
    "        self._pyaudio = pyaudio.PyAudio()\n",
    "        self._stream = self._pyaudio.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=1,\n",
    "            rate=self.sample_rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=512,\n",
    "            stream_callback=self.audio_callback,\n",
    "        )\n",
    "        self._stream.start_stream()\n",
    "\n",
    "        try:\n",
    "            await self.process_audio()\n",
    "        finally:\n",
    "            # cleanup\n",
    "            try:\n",
    "                self._stream.stop_stream()\n",
    "                self._stream.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                self._pyaudio.terminate()\n",
    "            except Exception:\n",
    "                pass\n",
    "            self.is_running = False\n",
    "            self.loop = None\n",
    "            self.async_queue = None\n",
    "\n",
    "    async def stop(self, transcribe_remaining: bool = True):\n",
    "        \"\"\"Signal the processing loop to stop and optionally transcribe remaining speech.\"\"\"\n",
    "        self.is_running = False\n",
    "        \n",
    "        # If we have buffered speech and want to transcribe it\n",
    "        if transcribe_remaining and self.is_speech_active and len(self.speech_chunks) > 0:\n",
    "            # Combine speech chunks\n",
    "            if len(self.speech_chunks) > 1:\n",
    "                buffer_copy = np.concatenate(self.speech_chunks)\n",
    "            else:\n",
    "                buffer_copy = self.speech_chunks[0].copy()\n",
    "            \n",
    "            # Clear state\n",
    "            self.is_speech_active = False\n",
    "            self.speech_chunks = []\n",
    "            self.speech_samples = 0\n",
    "            self.silence_counter = 0\n",
    "            \n",
    "            # Transcribe immediately (wait for completion)\n",
    "            await self._run_transcribe_and_callback(buffer_copy)\n",
    "        \n",
    "        # Wait for process_audio to exit\n",
    "        await asyncio.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import asyncio\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "def handle_transcript_chunk(text: str):\n",
    "    \"\"\"Callback called whenever a transcription chunk is ready.\"\"\"\n",
    "    if text.strip():\n",
    "        print(f\"\\n[TRANSCRIBED] {text}\")\n",
    "        all_chunks.append(text)\n",
    "\n",
    "async def test_live_transcription(duration_seconds: int = 10):\n",
    "    all_chunks.clear()\n",
    "    print(\"üé§ Speak in short sentences; pauses will trigger transcription.\")\n",
    "    transcriber = LiveTranscriber(\n",
    "        model_id=\"base\",          # keep whatever model id you use\n",
    "        language=\"en\",\n",
    "        on_transcript=handle_transcript_chunk,\n",
    "        vad_threshold=0.5,\n",
    "        min_speech_duration_ms=250,\n",
    "        min_silence_duration_ms=500,\n",
    "    )\n",
    "\n",
    "    # start the transcriber in the background\n",
    "    start_task = asyncio.create_task(transcriber.start())\n",
    "\n",
    "    try:\n",
    "        # run for given duration (you can interrupt with Ctrl+C)\n",
    "        await asyncio.sleep(duration_seconds)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user.\")\n",
    "    finally:\n",
    "        # ask the transcriber to stop and wait for it to finish\n",
    "        await transcriber.stop()\n",
    "\n",
    "        # wait for the start() task to exit cleanly\n",
    "        try:\n",
    "            await start_task\n",
    "        except Exception as e:\n",
    "            # if any error bubbled up from start/process_audio, show it\n",
    "            print(f\"Transcriber task ended with exception: {e}\")\n",
    "\n",
    "    print(\"\\nüìù Full transcript:\")\n",
    "    for i, t in enumerate(all_chunks, 1):\n",
    "        print(f\"{i}. {t}\")\n",
    "\n",
    "# run it\n",
    "await test_live_transcription(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
