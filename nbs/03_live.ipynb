{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: transcription.html\n",
    "title: Transcription\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Transcription\n",
    "\n",
    "This module implements a real-time speech-to-text system that: <br>\n",
    "- Listens to microphone input continuously <br>\n",
    "- Uses **Silero VAD** (Voice Activity Detection) to detect when you are speaking <br>\n",
    "- Automatically segments audio into utterances based on natural pauses <br>\n",
    "- Transcribes each utterance using **Faster-Whisper** (optimized Whisper implementation) <br>\n",
    "- Streams transcribed text chunks as they become available <br>\n",
    "\n",
    "The system is optimized for interactive applications that need responsive, chunked speech transcription with minimal latency.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "The implementation uses a multi-threaded, asynchronous architecture:\n",
    "\n",
    "1. **PyAudio Thread**: Captures raw audio from the microphone <br>\n",
    "2. **AsyncIO Queue**: Thread-safe bridge between audio callback and async processing <br>\n",
    "3. **Processing Loop**: Analyzes audio with VAD and triggers transcription <br>\n",
    "4. **Worker Threads**: Runs Whisper transcription without blocking the main loop <br>\n",
    "\n",
    "This design ensures audio capture never blocks or drops frames while maintaining responsive transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Device Selection\n",
    "\n",
    "The module imports only essential dependencies: <br>\n",
    "- **numpy**: Audio data manipulation and buffer management <br>\n",
    "- **torch**: Required for Silero VAD model <br>\n",
    "- **pyaudio**: Cross-platform audio I/O <br>\n",
    "- **faster_whisper**: Optimized Whisper implementation with CTranslate2 backend <br>\n",
    "- **asyncio**: Non-blocking audio processing <br>\n",
    "- **logging**: Debug and status output <br>\n",
    "\n",
    "### Device Selection Strategy\n",
    "\n",
    "The `get_device()` helper automatically selects the best available compute device:\n",
    "\n",
    "1. **CPU (forced)**: If `force_cpu=True` <br>\n",
    "2. **CUDA**: NVIDIA GPU acceleration (if available) <br>\n",
    "3. **MPS**: Apple Silicon Metal Performance Shaders (if available) <br>\n",
    "4. **CPU (fallback)**: Default fallback <br>\n",
    "\n",
    "For Whisper models, device selection impacts speed significantly: <br>\n",
    "- CPU: Slower but works everywhere <br>\n",
    "- CUDA: 5-10x faster on compatible NVIDIA GPUs <br>\n",
    "- MPS: 3-5x faster on M1/M2/M3 Macs <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/miniconda3/envs/tui_writer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#| include: false\n",
    "import logging\n",
    "import asyncio\n",
    "from typing import Optional, Callable\n",
    "from queue import Queue\n",
    "\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "def get_device(force_cpu: bool = False) -> str:\n",
    "    \"\"\"Pick best available device.\"\"\"\n",
    "    if force_cpu:\n",
    "        return \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def load_silero_vad():\n",
    "    \"\"\"Load Silero VAD model from torch hub.\"\"\"\n",
    "    try:\n",
    "        model, _utils = torch.hub.load(\n",
    "            repo_or_dir='snakers4/silero-vad',\n",
    "            model='silero_vad',\n",
    "            force_reload=False,\n",
    "            onnx=False\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to load Silero VAD: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó£Ô∏è Silero VAD (Voice Activity Detection)\n",
    "\n",
    "This small helper loads the **Silero VAD** model from `torch.hub`.\n",
    "\n",
    "Silero VAD is a lightweight neural model that outputs the **probability of speech** (0‚Äì1).  \n",
    "We‚Äôll use it to decide when the user is talking or has paused ‚Äî so we can send only meaningful speech chunks to Whisper.\n",
    "\n",
    "If Silero fails to load (e.g., offline), we just return `None` and handle it later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è LiveTranscriber ‚Äî Real-time Speech-to-Text\n",
    "\n",
    "The `LiveTranscriber` class captures live audio from your microphone, detects when you‚Äôre speaking using **Silero VAD**, and transcribes each spoken sentence using **Faster-Whisper** once you pause.\n",
    "\n",
    "It runs asynchronously, making it ideal for real-time interfaces like TUIs or assistants.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Parameters\n",
    "- **`model_id`** ‚Äì Whisper model to use (e.g. `\"tiny\"`, `\"base\"`, `\"small\"`).  \n",
    "- **`language`** ‚Äì Language code for transcription (default `\"en\"`).  \n",
    "- **`force_cpu`** ‚Äì Force CPU usage even if GPU is available.  \n",
    "- **`on_transcript`** ‚Äì Callback called with each transcribed text chunk.  \n",
    "- **`vad_threshold`** ‚Äì Silero confidence threshold (0.0‚Äì1.0, higher = stricter).  \n",
    "- **`min_speech_duration_ms`** ‚Äì Minimum length of speech to count as valid.  \n",
    "- **`min_silence_duration_ms`** ‚Äì How long silence must last before starting transcription.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Main Methods\n",
    "- **`start()`** ‚Äî Begins microphone capture and transcription loop (async).  \n",
    "- **`stop()`** ‚Äî Gracefully stops audio processing.  \n",
    "- **`process_audio()`** ‚Äî Runs continuously, detecting speech/silence and triggering transcription.  \n",
    "- **`_transcribe_chunk()`** ‚Äî Uses Whisper to transcribe one full utterance.  \n",
    "- **`_detect_speech_silero()`** ‚Äî Returns `True` if Silero VAD detects speech in the current chunk.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Summary\n",
    "- Audio is streamed in 32 ms chunks (512 samples at 16 kHz).  \n",
    "- Each chunk is passed to Silero VAD ‚Üí speech or silence.  \n",
    "- When silence lasts long enough, the buffered audio is sent to Whisper.  \n",
    "- The result is sent to your `on_transcript` callback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LiveTranscriber:\n",
    "    \"\"\"\n",
    "    Improved LiveTranscriber:\n",
    "      - uses asyncio.Queue for clean async consumption\n",
    "      - pushes audio from audio thread with loop.call_soon_threadsafe\n",
    "      - collects chunks in a list (no repeated np.append)\n",
    "      - does NOT append silence chunks into the sent buffer\n",
    "      - dispatches transcription to a thread and safely calls callbacks\n",
    "      - provides stop() for clean shutdown\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = \"openai/whisper-base\",\n",
    "        language: str = \"en\",\n",
    "        force_cpu: bool = False,\n",
    "        on_transcript: Optional[Callable[[str], None]] = None,\n",
    "        vad_threshold: float = 0.5,\n",
    "        min_speech_duration_ms: int = 250,\n",
    "        min_silence_duration_ms: int = 500,\n",
    "    ):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.on_transcript = on_transcript\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.language = language\n",
    "\n",
    "        self.sample_rate = 16000\n",
    "\n",
    "        # ASR model\n",
    "        self.device = get_device(force_cpu=force_cpu)\n",
    "        self.transcribe_model = WhisperModel(\n",
    "            self.model_id,\n",
    "            device=self.device,\n",
    "            compute_type=\"int8\" if self.device == \"cpu\" else \"float16\",\n",
    "        )\n",
    "\n",
    "        # VAD\n",
    "        self.vad_threshold = vad_threshold\n",
    "        self.silero_model = load_silero_vad()\n",
    "        if self.silero_model is None:\n",
    "            raise RuntimeError(\"Silero VAD failed to load. Cannot continue.\")\n",
    "\n",
    "        # thresholds in samples\n",
    "        self.min_speech_samples = int(self.sample_rate * min_speech_duration_ms / 1000)\n",
    "        self.min_silence_samples = int(self.sample_rate * min_silence_duration_ms / 1000)\n",
    "\n",
    "        # async queue and runtime vars (set in start())\n",
    "        self.async_queue: Optional[asyncio.Queue] = None\n",
    "        self.loop: Optional[asyncio.AbstractEventLoop] = None\n",
    "\n",
    "        # state used by process_audio\n",
    "        self.is_running = False\n",
    "        self.is_speech_active = False\n",
    "        self.speech_chunks: list[np.ndarray] = []  # collect speech chunks here (no np.append)\n",
    "        self.speech_samples = 0  # total samples currently in speech_chunks\n",
    "        self.silence_counter = 0  # in samples\n",
    "\n",
    "        # Pre-buffer to capture start of speech (keeps last 3 chunks before speech detected)\n",
    "        self.pre_buffer: list[np.ndarray] = []\n",
    "        self.pre_buffer_chunks = 3\n",
    "\n",
    "        self._pyaudio = None\n",
    "        self._stream = None\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"LiveTranscriber init (model={model_id}, device={self.device}, rate={self.sample_rate})\"\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # VAD & transcription helpers\n",
    "    # -------------------------\n",
    "    def _detect_speech_silero(self, audio_chunk: np.ndarray) -> bool:\n",
    "        \"\"\"Return True if Silero considers this chunk speech.\"\"\"\n",
    "        try:\n",
    "            audio_tensor = torch.from_numpy(audio_chunk).float()\n",
    "            prob = self.silero_model(audio_tensor, self.sample_rate).item()\n",
    "            return prob > self.vad_threshold\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Silero VAD error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _transcribe_chunk(self, audio_data: np.ndarray) -> str:\n",
    "        \"\"\"Blocking transcription call. Run in thread via asyncio.to_thread.\"\"\"\n",
    "        segments, _info = self.transcribe_model.transcribe(\n",
    "            audio_data,\n",
    "            language=self.language,\n",
    "            beam_size=1,\n",
    "            condition_on_previous_text=False,\n",
    "            vad_filter=False,\n",
    "        )\n",
    "        return \" \".join(s.text.strip() for s in segments).strip()\n",
    "\n",
    "    async def _run_transcribe_and_callback(self, buffer_copy: np.ndarray) -> None:\n",
    "        \"\"\"Run transcription in a worker thread and call user callback safely.\"\"\"\n",
    "        try:\n",
    "            text = await asyncio.to_thread(self._transcribe_chunk, buffer_copy)\n",
    "        except Exception as e:\n",
    "            self.logger.exception(f\"Transcription failed: {e}\")\n",
    "            return\n",
    "\n",
    "        if not text:\n",
    "            return\n",
    "\n",
    "        if not self.on_transcript:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            if asyncio.iscoroutinefunction(self.on_transcript):\n",
    "                await self.on_transcript(text)\n",
    "            else:\n",
    "                await asyncio.to_thread(self.on_transcript, text)\n",
    "        except Exception as e:\n",
    "            self.logger.exception(f\"on_transcript callback failed: {e}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # PyAudio callback -> async queue\n",
    "    # -------------------------\n",
    "    def audio_callback(self, in_data, frame_count, time_info, status):\n",
    "        \"\"\"Runs in PyAudio native thread. Put chunk into asyncio.Queue via loop.call_soon_threadsafe.\"\"\"\n",
    "        if status:\n",
    "            self.logger.debug(f\"Audio callback status: {status}\")\n",
    "        audio = np.frombuffer(in_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "        # the audio stream starts only after start() sets self.loop and self.async_queue\n",
    "        if self.loop and self.async_queue:\n",
    "            # safe: put_nowait from the audio thread via call_soon_threadsafe\n",
    "            self.loop.call_soon_threadsafe(self.async_queue.put_nowait, audio)\n",
    "        else:\n",
    "            # fallback: drop audio if not ready\n",
    "            self.logger.debug(\"Dropping audio: loop or async_queue not ready\")\n",
    "\n",
    "        return in_data, pyaudio.paContinue\n",
    "\n",
    "    # -------------------------\n",
    "    # Main async processing loop\n",
    "    # -------------------------\n",
    "    async def process_audio(self):\n",
    "        \"\"\"Consume audio chunks from self.async_queue and run VAD/chunking with minimal nesting.\"\"\"\n",
    "        assert self.async_queue is not None\n",
    "\n",
    "        while self.is_running:\n",
    "            # await next chunk (clean, no busy-sleep)\n",
    "            try:\n",
    "                chunk: np.ndarray = await self.async_queue.get()\n",
    "            except asyncio.CancelledError:\n",
    "                break\n",
    "\n",
    "            is_speech = self._detect_speech_silero(chunk)\n",
    "\n",
    "            if is_speech:\n",
    "                # start or extend a speech buffer (we only append speech chunks)\n",
    "                if not self.is_speech_active:\n",
    "                    # You just STARTED talking - include pre-buffer to capture beginning\n",
    "                    self.is_speech_active = True\n",
    "                    self.speech_chunks = self.pre_buffer.copy() + [chunk.copy()]    # Pre-buffer + current chunk\n",
    "                    self.speech_samples = sum(len(c) for c in self.speech_chunks)   # Count all samples\n",
    "                    self.silence_counter = 0                # Reset silence timer\n",
    "                    self.pre_buffer = []                    # Clear pre-buffer\n",
    "                else:\n",
    "                    # You're STILL talking\n",
    "                    self.speech_chunks.append(chunk)        # Add to buffer\n",
    "                    self.speech_samples += len(chunk)       # Count more samples\n",
    "                    self.silence_counter = 0                # Reset silence timer\n",
    "                continue    # Skip to next chunk\n",
    "\n",
    "            # chunk is silence\n",
    "            if not self.is_speech_active:\n",
    "                # You're still quiet - add to pre-buffer (rolling buffer)\n",
    "                self.pre_buffer.append(chunk.copy())\n",
    "                if len(self.pre_buffer) > self.pre_buffer_chunks:\n",
    "                    self.pre_buffer.pop(0)  # Remove oldest chunk\n",
    "                continue    # Skip to next chunk\n",
    "\n",
    "            # we were in speech; got a silence chunk -> count it\n",
    "            self.silence_counter += len(chunk)\n",
    "\n",
    "            if self.silence_counter < self.min_silence_samples:\n",
    "                # not enough silence yet to finalize\n",
    "                continue\n",
    "\n",
    "            # enough silence observed -> finalize this speech segment\n",
    "            if self.speech_samples >= self.min_speech_samples:\n",
    "\n",
    "                # 1. Combine all speech chunks into one audio buffer\n",
    "                if len(self.speech_chunks) > 1:\n",
    "                    buffer_copy = np.concatenate(self.speech_chunks)  # Multiple chunks\n",
    "                else:\n",
    "                    buffer_copy = self.speech_chunks[0].copy()        # Single chunk\n",
    "                \n",
    "                # 2. IMMEDIATELY reset state (so we can capture new speech)\n",
    "                self.is_speech_active = False\n",
    "                self.speech_chunks = []\n",
    "                self.speech_samples = 0\n",
    "                self.silence_counter = 0\n",
    "\n",
    "                # 3. Send to Whisper (in background, don't wait for it)\n",
    "                asyncio.create_task(self._run_transcribe_and_callback(buffer_copy))\n",
    "            else:\n",
    "                # Speech was too short (< 250ms), just ignore it\n",
    "                self.is_speech_active = False\n",
    "                self.speech_chunks = []\n",
    "                self.speech_samples = 0\n",
    "                self.silence_counter = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # Start / stop helpers\n",
    "    # -------------------------\n",
    "    async def start(self):\n",
    "        \"\"\"Start audio stream and processing loop. Returns when process_audio finishes (stop() called).\"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        self.loop = asyncio.get_running_loop()\n",
    "        self.async_queue = asyncio.Queue()\n",
    "        self.is_running = True\n",
    "\n",
    "        # start pyaudio stream\n",
    "        self._pyaudio = pyaudio.PyAudio()\n",
    "        self._stream = self._pyaudio.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=1,\n",
    "            rate=self.sample_rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=512,\n",
    "            stream_callback=self.audio_callback,\n",
    "        )\n",
    "        self._stream.start_stream()\n",
    "\n",
    "        try:\n",
    "            await self.process_audio()\n",
    "        finally:\n",
    "            # cleanup\n",
    "            try:\n",
    "                self._stream.stop_stream()\n",
    "                self._stream.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                self._pyaudio.terminate()\n",
    "            except Exception:\n",
    "                pass\n",
    "            self.is_running = False\n",
    "            self.loop = None\n",
    "            self.async_queue = None\n",
    "\n",
    "    async def stop(self, transcribe_remaining: bool = True):\n",
    "        \"\"\"Signal the processing loop to stop and optionally transcribe remaining speech.\"\"\"\n",
    "        self.is_running = False\n",
    "        \n",
    "        # If we have buffered speech and want to transcribe it\n",
    "        if transcribe_remaining and self.is_speech_active and len(self.speech_chunks) > 0:\n",
    "            # Combine speech chunks\n",
    "            if len(self.speech_chunks) > 1:\n",
    "                buffer_copy = np.concatenate(self.speech_chunks)\n",
    "            else:\n",
    "                buffer_copy = self.speech_chunks[0].copy()\n",
    "            \n",
    "            # Clear state\n",
    "            self.is_speech_active = False\n",
    "            self.speech_chunks = []\n",
    "            self.speech_samples = 0\n",
    "            self.silence_counter = 0\n",
    "            \n",
    "            # Transcribe immediately (wait for completion)\n",
    "            await self._run_transcribe_and_callback(buffer_copy)\n",
    "        \n",
    "        # Wait for process_audio to exit\n",
    "        await asyncio.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Live Transcription Test\n",
    "\n",
    "This test starts a 10-second live recording session using the `LiveTranscriber`.  \n",
    "Speak naturally in short sentences ‚Äî each pause will automatically trigger a transcription.  \n",
    "Each transcribed chunk is printed as soon as it‚Äôs ready, and all results are shown at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Speak in short sentences; pauses will trigger transcription.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jens/.cache/torch/hub/snakers4_silero-vad_master\n",
      "ALSA lib pcm_dsnoop.c:567:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRANSCRIBED] Okay, can you hear me?\n",
      "\n",
      "[TRANSCRIBED] That's nice.\n",
      "\n",
      "[TRANSCRIBED] What if now?\n",
      "\n",
      "[TRANSCRIBED] and now\n",
      "\n",
      "üìù Full transcript:\n",
      "1. Okay, can you hear me?\n",
      "2. That's nice.\n",
      "3. What if now?\n",
      "4. and now\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import asyncio\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "def handle_transcript_chunk(text: str):\n",
    "    \"\"\"Callback called whenever a transcription chunk is ready.\"\"\"\n",
    "    if text.strip():\n",
    "        print(f\"\\n[TRANSCRIBED] {text}\")\n",
    "        all_chunks.append(text)\n",
    "\n",
    "async def test_live_transcription(duration_seconds: int = 10):\n",
    "    all_chunks.clear()\n",
    "    print(\"üé§ Speak in short sentences; pauses will trigger transcription.\")\n",
    "    transcriber = LiveTranscriber(\n",
    "        model_id=\"base\",          # keep whatever model id you use\n",
    "        language=\"en\",\n",
    "        on_transcript=handle_transcript_chunk,\n",
    "        vad_threshold=0.5,\n",
    "        min_speech_duration_ms=250,\n",
    "        min_silence_duration_ms=500,\n",
    "    )\n",
    "\n",
    "    # start the transcriber in the background\n",
    "    start_task = asyncio.create_task(transcriber.start())\n",
    "\n",
    "    try:\n",
    "        # run for given duration (you can interrupt with Ctrl+C)\n",
    "        await asyncio.sleep(duration_seconds)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user.\")\n",
    "    finally:\n",
    "        # ask the transcriber to stop and wait for it to finish\n",
    "        await transcriber.stop()\n",
    "\n",
    "        # wait for the start() task to exit cleanly\n",
    "        try:\n",
    "            await start_task\n",
    "        except Exception as e:\n",
    "            # if any error bubbled up from start/process_audio, show it\n",
    "            print(f\"Transcriber task ended with exception: {e}\")\n",
    "\n",
    "    print(\"\\nüìù Full transcript:\")\n",
    "    for i, t in enumerate(all_chunks, 1):\n",
    "        print(f\"{i}. {t}\")\n",
    "\n",
    "# run it\n",
    "await test_live_transcription(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
