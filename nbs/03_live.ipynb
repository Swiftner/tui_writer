{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé§ LiveTranscriber ‚Äî Real-time Speech Transcription with VAD\n",
    "\n",
    "This notebook implements a **live voice-to-text system** that:\n",
    "- Listens to your microphone input in real-time  \n",
    "- Uses **Silero VAD** (Voice Activity Detection) to detect when you're speaking  \n",
    "- Automatically splits audio into *utterances* based on pauses  \n",
    "- Transcribes each utterance using **Faster-Whisper** (OpenAI Whisper variant)  \n",
    "- Streams the transcribed text chunks as they become available\n",
    "\n",
    "It‚Äôs optimized for local apps (like TUIs or assistants) that need responsive, chunked speech transcription.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Imports and Device Selection\n",
    "\n",
    "We import only the necessary libraries:\n",
    "- `numpy`, `torch`, `pyaudio` for audio and model operations  \n",
    "- `faster_whisper` for the actual speech-to-text model  \n",
    "- `asyncio` for non-blocking background processing  \n",
    "- `logging` for debug output  \n",
    "\n",
    "We also define a small helper to pick the **best available compute device**:\n",
    "- CUDA (GPU)\n",
    "- MPS (Apple Silicon)\n",
    "- or CPU (fallback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/miniconda3/envs/tui_writer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import logging\n",
    "import asyncio\n",
    "from typing import Optional, Callable\n",
    "from queue import Queue\n",
    "\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "def get_device(force_cpu: bool = False) -> str:\n",
    "    \"\"\"Pick best available device.\"\"\"\n",
    "    if force_cpu:\n",
    "        return \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def load_silero_vad():\n",
    "    \"\"\"Load Silero VAD model from torch hub.\"\"\"\n",
    "    try:\n",
    "        model, _utils = torch.hub.load(\n",
    "            repo_or_dir='snakers4/silero-vad',\n",
    "            model='silero_vad',\n",
    "            force_reload=False,\n",
    "            onnx=False\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to load Silero VAD: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó£Ô∏è Silero VAD (Voice Activity Detection)\n",
    "\n",
    "This small helper loads the **Silero VAD** model from `torch.hub`.\n",
    "\n",
    "Silero VAD is a lightweight neural model that outputs the **probability of speech** (0‚Äì1).  \n",
    "We‚Äôll use it to decide when the user is talking or has paused ‚Äî so we can send only meaningful speech chunks to Whisper.\n",
    "\n",
    "If Silero fails to load (e.g., offline), we just return `None` and handle it later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è LiveTranscriber ‚Äî Real-time Speech-to-Text\n",
    "\n",
    "The `LiveTranscriber` class captures live audio from your microphone, detects when you‚Äôre speaking using **Silero VAD**, and transcribes each spoken sentence using **Faster-Whisper** once you pause.\n",
    "\n",
    "It runs asynchronously, making it ideal for real-time interfaces like TUIs or assistants.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Parameters\n",
    "- **`model_id`** ‚Äì Whisper model to use (e.g. `\"tiny\"`, `\"base\"`, `\"small\"`).  \n",
    "- **`language`** ‚Äì Language code for transcription (default `\"en\"`).  \n",
    "- **`force_cpu`** ‚Äì Force CPU usage even if GPU is available.  \n",
    "- **`on_transcript`** ‚Äì Callback called with each transcribed text chunk.  \n",
    "- **`vad_threshold`** ‚Äì Silero confidence threshold (0.0‚Äì1.0, higher = stricter).  \n",
    "- **`min_speech_duration_ms`** ‚Äì Minimum length of speech to count as valid.  \n",
    "- **`min_silence_duration_ms`** ‚Äì How long silence must last before starting transcription.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Main Methods\n",
    "- **`start()`** ‚Äî Begins microphone capture and transcription loop (async).  \n",
    "- **`stop()`** ‚Äî Gracefully stops audio processing.  \n",
    "- **`process_audio()`** ‚Äî Runs continuously, detecting speech/silence and triggering transcription.  \n",
    "- **`_transcribe_chunk()`** ‚Äî Uses Whisper to transcribe one full utterance.  \n",
    "- **`_detect_speech_silero()`** ‚Äî Returns `True` if Silero VAD detects speech in the current chunk.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Summary\n",
    "- Audio is streamed in 32 ms chunks (512 samples at 16 kHz).  \n",
    "- Each chunk is passed to Silero VAD ‚Üí speech or silence.  \n",
    "- When silence lasts long enough, the buffered audio is sent to Whisper.  \n",
    "- The result is sent to your `on_transcript` callback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LiveTranscriber:\n",
    "    \"\"\"Live audio transcription for TUI applications using PyAudio and Whisper with Silero VAD-based chunking.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_id: str = \"openai/whisper-base\",\n",
    "        language: str = \"en\",\n",
    "        force_cpu: bool = False,\n",
    "        on_transcript: Optional[Callable[[str], None]] = None,\n",
    "        vad_threshold: float = 0.5,\n",
    "        min_speech_duration_ms: int = 250,\n",
    "        min_silence_duration_ms: int = 500,\n",
    "    ):\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.on_transcript = on_transcript\n",
    "        \n",
    "        self.model_id = model_id\n",
    "        self.language = language\n",
    "\n",
    "        # Fixed 16 kHz sample rate (required by Silero + Whisper)\n",
    "        self.sample_rate = 16000\n",
    "        \n",
    "        # Device + ASR model\n",
    "        self.device = get_device(force_cpu=force_cpu)\n",
    "        self.transcribe_model = WhisperModel(\n",
    "            self.model_id,\n",
    "            device=self.device,\n",
    "            compute_type=\"int8\" if self.device == \"cpu\" else \"float16\", # use \"float32\" on MPS if needed\n",
    "        )\n",
    "\n",
    "\n",
    "        # Load Silero VAD\n",
    "        self.vad_threshold = vad_threshold\n",
    "        self.silero_model = load_silero_vad()\n",
    "        if self.silero_model is None:\n",
    "            raise RuntimeError(\"Silero VAD failed to load. Cannot continue.\")\n",
    "\n",
    "        # Thresholds (in samples)\n",
    "        self.min_speech_samples = int(self.sample_rate * min_speech_duration_ms / 1000)\n",
    "        self.min_silence_samples = int(self.sample_rate * min_silence_duration_ms / 1000)\n",
    "\n",
    "        # Buffers and state\n",
    "        self.audio_queue: \"Queue[np.ndarray]\" = Queue()\n",
    "        self.is_running = False\n",
    "\n",
    "        self.is_speech_active = False\n",
    "        self.speech_buffer = np.array([], dtype=np.float32)\n",
    "        self.silence_counter = 0\n",
    "\n",
    "        self.logger.info(f\"Initialized LiveTranscriber (model={model_id}, device={self.device}, sample_rate=16kHz, VAD=Silero)\")\n",
    "    \n",
    "    def _detect_speech_silero(self, audio_chunk: np.ndarray) -> bool:\n",
    "        \"\"\"Return True if speech detected; False on low prob or on error.\"\"\"\n",
    "        try:\n",
    "            audio_tensor = torch.from_numpy(audio_chunk).float()\n",
    "            prob = self.silero_model(audio_tensor, self.sample_rate).item()\n",
    "            return prob > self.vad_threshold\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Silero VAD error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _transcribe_chunk(self, audio_data: np.ndarray) -> str:\n",
    "        segments, _info = self.transcribe_model.transcribe(\n",
    "            audio_data,\n",
    "            language=self.language,\n",
    "            beam_size=1,\n",
    "            condition_on_previous_text=False,\n",
    "            vad_filter=True, # we already did VAD; set True if you want extra internal filtering\n",
    "            vad_parameters=dict(\n",
    "                threshold=0.4,\n",
    "                min_speech_duration_ms=self.min_speech_samples * 1000 // self.sample_rate,\n",
    "                max_speech_duration_s=float(\"inf\"),\n",
    "                min_silence_duration_ms=200,\n",
    "            ),\n",
    "        )\n",
    "        return \" \".join(s.text.strip() for s in segments).strip()\n",
    "    \n",
    "    def audio_callback(self, in_data, frame_count, time_info, status):\n",
    "        \"\"\"Called automatically by PyAudio for each audio frame.\"\"\"\n",
    "        if status:\n",
    "            self.logger.debug(f\"Audio callback status: {status}\")\n",
    "        audio = np.frombuffer(in_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "        self.audio_queue.put(audio)\n",
    "        return in_data, pyaudio.paContinue\n",
    "    \n",
    "    async def process_audio(self):\n",
    "        \"\"\"Process queued audio in real-time with VAD chunking.\"\"\"\n",
    "        while self.is_running:\n",
    "            if self.audio_queue.empty():\n",
    "                await asyncio.sleep(0.01)\n",
    "                continue\n",
    "\n",
    "            chunk = self.audio_queue.get()\n",
    "            if self._detect_speech_silero(chunk):\n",
    "                if not self.is_speech_active:\n",
    "                    self.is_speech_active = True\n",
    "                    self.speech_buffer = chunk.copy()\n",
    "                    self.silence_counter = 0\n",
    "                else:\n",
    "                    self.speech_buffer = np.append(self.speech_buffer, chunk)\n",
    "                    self.silence_counter = 0\n",
    "            else:\n",
    "                if self.is_speech_active:\n",
    "                    self.silence_counter += len(chunk)\n",
    "                    self.speech_buffer = np.append(self.speech_buffer, chunk)\n",
    "\n",
    "                    if self.silence_counter >= self.min_silence_samples:\n",
    "                        if len(self.speech_buffer) >= self.min_speech_samples:\n",
    "                            text = await asyncio.to_thread(self._transcribe_chunk, self.speech_buffer)\n",
    "                            if text and self.on_transcript:\n",
    "                                if asyncio.iscoroutinefunction(self.on_transcript):\n",
    "                                    await self.on_transcript(text)\n",
    "                                else:\n",
    "                                    self.on_transcript(text)\n",
    "                        # reset\n",
    "                        self.is_speech_active = False\n",
    "                        self.speech_buffer = np.array([], dtype=np.float32)\n",
    "                        self.silence_counter = 0\n",
    "    \n",
    "    async def start(self):\n",
    "        \"\"\"Start recording and transcription loop.\"\"\"\n",
    "        self.is_running = True\n",
    "        audio = pyaudio.PyAudio()\n",
    "        try:\n",
    "            stream = audio.open(\n",
    "                format=pyaudio.paInt16,\n",
    "                channels=1,\n",
    "                rate=self.sample_rate,\n",
    "                input=True,\n",
    "                frames_per_buffer=512,\n",
    "                stream_callback=self.audio_callback,\n",
    "            )\n",
    "            stream.start_stream()\n",
    "            try:\n",
    "                await self.process_audio()\n",
    "            finally:\n",
    "                stream.stop_stream()\n",
    "                stream.close()\n",
    "        finally:\n",
    "            audio.terminate()\n",
    "\n",
    "    def stop(self):\n",
    "        self.is_running = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Live Transcription Test\n",
    "\n",
    "This test starts a 10-second live recording session using the `LiveTranscriber`.  \n",
    "Speak naturally in short sentences ‚Äî each pause will automatically trigger a transcription.  \n",
    "Each transcribed chunk is printed as soon as it‚Äôs ready, and all results are shown at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Speak in short sentences; pauses will trigger transcription.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jens/.cache/torch/hub/snakers4_silero-vad_master\n",
      "ALSA lib pcm_dsnoop.c:567:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRANSCRIBED] Hello everyone.\n",
      "\n",
      "[TRANSCRIBED] Today I will be teaching you about something.\n",
      "\n",
      "[TRANSCRIBED] I don't know what that is.\n",
      "\n",
      "üìù Full transcript:\n",
      "1. Hello everyone.\n",
      "2. Today I will be teaching you about something.\n",
      "3. I don't know what that is.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import asyncio\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "def handle_transcript_chunk(text: str):\n",
    "    \"\"\"Callback called whenever a transcription chunk is ready.\"\"\"\n",
    "    if text.strip():\n",
    "        print(f\"\\n[TRANSCRIBED] {text}\")\n",
    "        all_chunks.append(text)\n",
    "\n",
    "async def test_live_transcription(duration_seconds: int = 10):\n",
    "    print(\"üé§ Speak in short sentences; pauses will trigger transcription.\")\n",
    "    transcriber = LiveTranscriber(\n",
    "        model_id=\"tiny\",\n",
    "        language=\"en\",\n",
    "        on_transcript=handle_transcript_chunk,\n",
    "        vad_threshold=0.5,\n",
    "        min_speech_duration_ms=250,\n",
    "        min_silence_duration_ms=500,\n",
    "    )\n",
    "    task = asyncio.create_task(transcriber.start())\n",
    "    try:\n",
    "        await asyncio.sleep(duration_seconds)\n",
    "    finally:\n",
    "        transcriber.stop()\n",
    "        await asyncio.sleep(0.3)\n",
    "        task.cancel()\n",
    "        try:\n",
    "            await task\n",
    "        except asyncio.CancelledError:\n",
    "            pass\n",
    "\n",
    "    print(\"\\nüìù Full transcript:\")\n",
    "    for i, t in enumerate(all_chunks, 1):\n",
    "        print(f\"{i}. {t}\")\n",
    "\n",
    "await test_live_transcription(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
