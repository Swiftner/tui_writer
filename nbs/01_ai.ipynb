{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  AI-Powered Text Operations\n",
    "\n",
    "Multi-provider AI operations using the `lisette` library for flexible text editing and analysis.\n",
    "\n",
    "**Features:**\n",
    "- Multi-model support (Gemini, Claude, OpenAI, etc.)\n",
    "- Natural language instructions for text editing\n",
    "- Web search capabilities for real-time information\n",
    "- Transcript summarization and improvement\n",
    "- Change explanation and analysis\n",
    "- Efficient conversation management (single assistant message + cumulative user instructions)\n",
    "- Natural language support for commands like \"change it back\" or \"undo that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "from typing import List, Dict, Literal, Union\n",
    "from pydantic import BaseModel, ConfigDict, Field, model_validator\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import json\n",
    "from lisette import Chat\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Core AI Functions\n",
    "\n",
    "AI-powered operations using lisette's flexible multi-provider interface.\n",
    "\n",
    "**Main Functions:**\n",
    "- `ai_chat()` â€” General-purpose AI chat with multi-model support\n",
    "- `summarize_transcript()` â€” Generate concise summaries\n",
    "- `explain_edits()` â€” Natural language explanation of changes\n",
    "- `improve_transcript()` â€” Flexible text improvement with custom instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# --- Replace all ------------------------------------------------------------\n",
    "\n",
    "class ReplaceAllOp(BaseModel):\n",
    "    \"\"\"Represents a 'replace all' text operation.\"\"\"\n",
    "    op: Literal[\"replace_all\"]\n",
    "    find: str = Field(..., min_length=1)\n",
    "    replace: str = Field(..., min_length=0)\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "\n",
    "# --- Regex replace ------------------------------------------------------------\n",
    "\n",
    "class RegexReplaceOp(BaseModel):\n",
    "    \"\"\"Represents a regex-based find/replace operation.\"\"\"\n",
    "    op: Literal[\"regex_replace\"]\n",
    "    pattern: str = Field(..., min_length=1)\n",
    "    replacement: str = Field(..., min_length=0)\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def _validate_regex(cls, v: \"RegexReplaceOp\"):\n",
    "        # Precompile regex to ensure it's valid\n",
    "        try:\n",
    "            re.compile(v.pattern)\n",
    "        except re.error as e:\n",
    "            raise ValueError(f\"Invalid regex pattern: {e}\") from e\n",
    "        return v\n",
    "\n",
    "# --- Insert at absolute position ---------------------------------------------\n",
    "\n",
    "class InsertAtOp(BaseModel):\n",
    "    \"\"\"Insert text at an absolute character position (0-indexed).\"\"\"\n",
    "    op: Literal[\"insert_at\"]\n",
    "    text: str = Field(..., min_length=1)\n",
    "    position: int = Field(..., ge=0)\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "# --- Insert after marker ------------------------------------------------------\n",
    "\n",
    "class InsertAfterOp(BaseModel):\n",
    "    \"\"\"Insert text after the first occurrence of a marker string.\"\"\"\n",
    "    op: Literal[\"insert_after\"]\n",
    "    text: str = Field(..., min_length=1)\n",
    "    after: str = Field(..., min_length=1)\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "# --- Delete -------------------------------------------------------------------\n",
    "\n",
    "class DeleteOp(BaseModel):\n",
    "    \"\"\"Delete exact text (first or all occurrences).\"\"\"\n",
    "    op: Literal[\"delete\"]\n",
    "    text: str = Field(..., min_length=1)\n",
    "    all_occurrences: bool = False\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "\n",
    "# --- Edit plan container ------------------------------------------------------\n",
    "\n",
    "class EditPlan(BaseModel):\n",
    "    \"\"\"Represents a list of text operations to apply sequentially.\"\"\"\n",
    "    ops: List[\n",
    "        Union[\n",
    "            ReplaceAllOp,\n",
    "            RegexReplaceOp,\n",
    "            InsertAtOp,\n",
    "            InsertAfterOp,\n",
    "            DeleteOp,\n",
    "        ]\n",
    "    ]\n",
    "    model_config = ConfigDict(extra=\"forbid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§° Conversation Management\n",
    "\n",
    "The AI conversation uses a hybrid context pattern for efficiency.\n",
    "\n",
    "**Session State:**\n",
    "- `_messages` â€” conversation history\n",
    "- `_current` â€” current transcript after applied edits\n",
    "\n",
    "**Structure:**\n",
    "- **System message:** defines AI role and available operations\n",
    "- **Assistant message:** contains current transcript (updated after each edit)\n",
    "- **User messages:** cumulative instruction history\n",
    "\n",
    "**Example after 2 edits:**\n",
    "```json\n",
    "[\n",
    "  {\"role\": \"system\", \"content\": \"You are a precise text editor...\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Here is the current transcript:\\nI met oscar on Monday.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Instruction: Change him to oscar\"},\n",
    "  {\"role\": \"user\", \"content\": \"Instruction: Change yesterday to on Monday\"}\n",
    "]\n",
    "```\n",
    "\n",
    "**Functions:**\n",
    "- `_new_conversation(transcript)` â€” initializes conversation with system and assistant messages\n",
    "- `_set_current_transcript(new_transcript)` â€” updates assistant message with latest transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# --- session state (module-level) ---\n",
    "_messages: List[Dict[str, str]] | None = None\n",
    "_current: str | None = None\n",
    "\n",
    "def _new_conversation(transcript: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Create a new message list with system + assistant context.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a precise text editor that outputs ONLY valid JSON matching the EditPlan schema.\\n\\n\"\n",
    "                \"Available operations:\\n\"\n",
    "                \"1. replace_all â€” exact literal text only (no regex)\\n\"\n",
    "                \"   fields:\\n\"\n",
    "                \"       - find: the exact text to replace\\n\"\n",
    "                \"       - replace: replacement text for every occurrence\\n\\n\"\n",
    "                \"2. regex_replace - pattern-based replacements (e.g., dates)\\n\"\n",
    "                \"   fields:\\n\"\n",
    "                \"       - pattern: regex pattern to match (e.g., (\\\\d{4})-(\\\\d{2})-(\\\\d{2}) for dates)\\n\"\n",
    "                \"       - replacement: replacement string using \\\\1, \\\\2 for capture groups\\n\\n\"\n",
    "                \"3. insert_at â€” insert text at an absolute index (0 = start)\\n\"\n",
    "                \"   fields:\\n\"\n",
    "                \"       - text: text to insert\\n\"\n",
    "                \"       - position: integer index to insert at\\n\\n\"\n",
    "                \"4. insert_after â€” insert text after a marker\\n\"\n",
    "                \"   fields:\\n\"\n",
    "                \"       - text: text to insert\\n\"\n",
    "                \"       - after: insert after the first occurrence of this string\\n\"\n",
    "                \"       (ALWAYS provide a space in the string if needed when doing insert)\\n\\n\"\n",
    "                \"5. delete â€” remove exact text\\n\"\n",
    "                \"   fields:\\n\"\n",
    "                \"       - text: the exact text to remove\\n\"\n",
    "                \"       - all_occurrences: true = remove all, false = only first (default false)\\n\\n\"\n",
    "                \"RULES:\\n\"\n",
    "                \"- If you see regex patterns or date formats, you MUST use regex_replace, NOT replace_all!\\n\"\n",
    "                \"- When interpreting natural or spoken language, infer the user's intent precisely and map it to the correct fields.\\n\"\n",
    "                \"- ALWAYS provide a space in text to insert if needed.\\n\"\n",
    "                \"- Respond ONLY with valid JSON following the EditPlan schema.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"Current text to edit:\\n{transcript}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "def _set_current_transcript(new_transcript: str) -> None:\n",
    "    global _messages\n",
    "    # replace the single assistant transcript message\n",
    "    for m in _messages:\n",
    "        if m.get(\"role\") == \"assistant\":\n",
    "            m[\"content\"] = f\"Current text to edit:\\n{new_transcript}\"\n",
    "            return\n",
    "    # Fallback: insert one if missing\n",
    "    _messages.insert(1, {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Current text to edit:\\n{new_transcript}\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Core Functions\n",
    "\n",
    "**`_plan_edits(instruction, model)`**\n",
    "- Appends user instruction to conversation\n",
    "- Calls LLM with `response_format=EditPlan` for structured output\n",
    "- Returns parsed `EditPlan` object\n",
    "\n",
    "**`_apply_plan(transcript, plan)`**\n",
    "- Applies all operations in `EditPlan` sequentially to the transcript\n",
    "- Supports: `replace_all`, `regex_replace`, `insert_at`, `insert_after`, `delete`\n",
    "- Returns updated transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def _plan_edits(instruction: str, model: str = \"gemini/gemini-2.5-flash\") -> EditPlan:\n",
    "    \"\"\"\n",
    "    Append a user instruction, call the model with structured output, and return the parsed plan.\n",
    "    \"\"\"\n",
    "    global _messages\n",
    "\n",
    "    # Add the new instruction to the conversation\n",
    "    _messages.append({\"role\": \"user\", \"content\": f\"Instruction: {instruction}\"})\n",
    "\n",
    "    # Use lisette to get structured JSON response\n",
    "    chat = Chat(model, response_format=\"json\")\n",
    "    \n",
    "    # Format messages for lisette (convert our format to lisette's expected format)\n",
    "    response = chat(messages=_messages, temperature=0)\n",
    "    \n",
    "    # Extract the JSON content from response\n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    # Parse JSON and validate with Pydantic\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "        plan = EditPlan.model_validate(data)\n",
    "        return plan\n",
    "    except (json.JSONDecodeError, Exception) as e:\n",
    "        raise RuntimeError(f\"Failed to parse model response as EditPlan: {e}\\nResponse: {content}\")\n",
    "\n",
    "\n",
    "def _apply_plan(transcript: str, plan: EditPlan) -> str:\n",
    "    \"\"\"\n",
    "    Apply all operations from the EditPlan to the given transcript.\n",
    "    \"\"\"\n",
    "    updated = transcript\n",
    "    for op in plan.ops:\n",
    "        if op.op == \"replace_all\":\n",
    "            updated = updated.replace(op.find, op.replace)\n",
    "        elif op.op == \"regex_replace\":\n",
    "            updated = re.sub(op.pattern, op.replacement, updated)\n",
    "        elif op.op == \"insert_at\":\n",
    "            pos = max(0, min(op.position, len(updated)))\n",
    "            updated = updated[:pos] + op.text + updated[pos:]\n",
    "        elif op.op == \"insert_after\":\n",
    "            idx = updated.find(op.after)\n",
    "            if idx != -1:\n",
    "                insert_pos = idx + len(op.after)\n",
    "                updated = updated[:insert_pos] + op.text + updated[insert_pos:]\n",
    "        elif op.op == \"delete\":\n",
    "            if op.all_occurrences:\n",
    "                updated = updated.replace(op.text, \"\")\n",
    "            else:\n",
    "                # Delete first occurrence only\n",
    "                idx = updated.find(op.text)\n",
    "                if idx != -1:\n",
    "                    updated = updated[:idx] + updated[idx + len(op.text):]\n",
    "    return updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Œ Public API\n",
    "\n",
    "Functions for managing edit sessions and applying instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def has_session() -> bool:\n",
    "    \"\"\"Return True if an edit session is initialized.\"\"\"\n",
    "    return _messages is not None and _current is not None\n",
    "\n",
    "def start_session(initial_transcript: str) -> str:\n",
    "    \"\"\"Seed a new session with the initial transcript and return it.\"\"\"\n",
    "    global _messages, _current\n",
    "    _current = initial_transcript\n",
    "    _messages = _new_conversation(initial_transcript)\n",
    "    return _current\n",
    "\n",
    "def apply_instruction(instruction: str, model: str = \"gemini/gemini-2.5-flash\") -> str:\n",
    "    \"\"\"Apply an instruction to the current transcript and return the updated text.\"\"\"\n",
    "    global _current\n",
    "    if not has_session():\n",
    "        raise RuntimeError(\"No session. Call start_session() first.\")\n",
    "    plan = _plan_edits(instruction, model)\n",
    "    _current = _apply_plan(_current, plan)\n",
    "    _set_current_transcript(_current)\n",
    "    return _current\n",
    "\n",
    "def current_transcript() -> str:\n",
    "    \"\"\"Get the latest edited transcript (or '' if none).\"\"\"\n",
    "    return _current or \"\"\n",
    "\n",
    "def reset_session() -> None:\n",
    "    \"\"\"Clear session state.\"\"\"\n",
    "    global _messages, _current\n",
    "    _messages, _current = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Lisette Integration (Multi-Model AI)\n",
    "\n",
    "Additional AI capabilities using the `lisette` library for flexible, multi-provider AI operations.\n",
    "\n",
    "**Use Cases:**\n",
    "- General chat and Q&A with multiple AI providers (Gemini, Claude, OpenRouter models, etc.)\n",
    "- Web search-enabled queries\n",
    "- Transcript summarization and analysis\n",
    "- Explaining edits between versions\n",
    "\n",
    "**Note:** All AI operations in this module use lisette for consistent multi-provider support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def ai_chat(\n",
    "    prompt: str, \n",
    "    model: str = \"gemini/gemini-2.5-flash\", \n",
    "    enable_search: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    General-purpose AI chat using lisette for multi-provider support.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Question or instruction for the AI\n",
    "        model: Model identifier (e.g., \"gemini/gemini-2.5-flash\", \"claude-sonnet-4-20250514\", \"gpt-4o\")\n",
    "        enable_search: Whether to enable web search capabilities\n",
    "    \n",
    "    Returns:\n",
    "        AI response text\n",
    "        \n",
    "    Example:\n",
    "        >>> response = ai_chat(\"What is the capital of Norway?\", enable_search=True)\n",
    "        >>> print(response)\n",
    "    \"\"\"\n",
    "    search_level = \"l\" if enable_search else None\n",
    "    chat = Chat(model, search=search_level)\n",
    "    response = chat(prompt)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def summarize_transcript(\n",
    "    transcript: str, \n",
    "    model: str = \"gemini/gemini-2.5-flash\",\n",
    "    max_words: int = 100\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a concise summary of a transcript.\n",
    "    \n",
    "    Args:\n",
    "        transcript: The text to summarize\n",
    "        model: AI model to use\n",
    "        max_words: Maximum words for the summary\n",
    "        \n",
    "    Returns:\n",
    "        Summary text\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Summarize this transcript in {max_words} words or less:\n",
    "\n",
    "{transcript}\n",
    "\n",
    "Provide a clear, concise summary.\"\"\"\n",
    "    return ai_chat(prompt, model)\n",
    "\n",
    "\n",
    "def explain_edits(original: str, edited: str, model: str = \"gemini/gemini-2.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Get an AI explanation of what changed between two text versions.\n",
    "    \n",
    "    Args:\n",
    "        original: Original text\n",
    "        edited: Edited/modified text\n",
    "        model: AI model to use\n",
    "        \n",
    "    Returns:\n",
    "        Natural language explanation of changes\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Compare these two versions and explain what changed:\n",
    "\n",
    "ORIGINAL:\n",
    "{original}\n",
    "\n",
    "EDITED:\n",
    "{edited}\n",
    "\n",
    "Provide a brief, clear explanation of the changes made.\"\"\"\n",
    "    return ai_chat(prompt, model)\n",
    "\n",
    "\n",
    "def improve_transcript(\n",
    "    transcript: str,\n",
    "    instructions: str = \"Fix grammar, punctuation, and clarity while preserving meaning\",\n",
    "    model: str = \"gemini/gemini-2.5-flash\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Use AI to improve transcript quality with flexible instructions.\n",
    "    \n",
    "    Args:\n",
    "        transcript: Text to improve\n",
    "        instructions: How to improve it (grammar, clarity, formality, etc.)\n",
    "        model: AI model to use\n",
    "        \n",
    "    Returns:\n",
    "        Improved transcript text\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"{instructions}\n",
    "\n",
    "TEXT:\n",
    "{transcript}\n",
    "\n",
    "Return ONLY the improved text, no explanations.\"\"\"\n",
    "    return ai_chat(prompt, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Lisette Examples\n",
    "\n",
    "Practical examples showing how to use the lisette-powered functions for various AI tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– AI: Python is a highly versatile, general-purpose programming language, so it doesn't have *one single* primary use case in the way a specialized language might. However, if we were to identify its most prominent and impactful areas where it truly shines and dominates, they would be:\n",
      "\n",
      "1.  **Data Science, Machine Learning, and Artificial Intelligence (AI):** This is arguably Python's strongest and most defining primary use case today.\n",
      "    *   **Why:** Its rich ecosystem of powerful libraries like NumPy (numerical computing), Pandas (data manipulation and analysis), SciPy (scientific computing), Matplotlib/Seaborn (data visualization), Scikit-learn (machine learning), TensorFlow, and PyTorch (deep learning) makes it the go-to language for data scientists, analysts, and AI researchers.\n",
      "    *   **What it's used for:** Data cleaning, analysis, visualization, building predictive models, developing AI algorithms, natural language processing, computer vision, and more.\n",
      "\n",
      "2.  **Web Development (Backend):** Python is extremely popular for building the backend of web applications.\n",
      "    *   **Why:** Frameworks like Django (for large, complex, database-driven applications) and Flask (for smaller, more lightweight APIs and microservices) provide robust tools and structures.\n",
      "    *   **What it's used for:** Handling server-side logic, interacting with databases, managing user authentication, processing requests, and building APIs.\n",
      "\n",
      "3.  **Automation, Scripting, and DevOps:** Python's readability, extensive standard library, and cross-platform compatibility make it excellent for automating repetitive tasks.\n",
      "    *   **Why:** It can easily interact with operating systems, file systems, networks, and other applications.\n",
      "    *   **What it's used for:** System administration, network configuration, creating build/deployment scripts, data parsing, web scraping, and general task automation.\n",
      "\n",
      "**In summary, while Python can be used for almost anything, its primary impact and widespread adoption are most evident in:**\n",
      "\n",
      "*   **Data-driven fields (Data Science, ML, AI)**\n",
      "*   **Backend web development**\n",
      "*   **Automation and scripting**\n",
      "\n",
      "Its simplicity, readability, vast library ecosystem, and strong community support are key factors contributing to its dominance in these areas.\n",
      "ðŸŒ AI with search: In Oslo, Norway, the weather today, Friday, October 17, 2025, is mostly sunny with a temperature of 43Â°F (6Â°C), feeling like 43Â°F (6Â°C). The humidity is around 65%, and there is approximately a 0% chance of rain.\n",
      "\n",
      "The forecast for the rest of the day indicates sunny conditions, changing to clear with periodic clouds at night. There is a 35% chance of snow during the day. Temperatures are expected to range between 32Â°F (0Â°C) and 45Â°F (7Â°C), with humidity around 74%.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Example 1: Basic AI Chat\n",
    "\n",
    "# Simple question without web search\n",
    "response = ai_chat(\"What is Python's primary use case?\")\n",
    "print(\"ðŸ¤– AI:\", response)\n",
    "\n",
    "# Question with web search enabled\n",
    "response = ai_chat(\"What is the weather in Oslo today?\", enable_search=True)\n",
    "print(\"ðŸŒ AI with search:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Summary: The speaker had a productive day, buying groceries, hardware, and picking up a prescription. They also met a friend at a coffee shop to discuss her new job.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Example 2: Summarize a Transcript\n",
    "\n",
    "long_transcript = \"\"\"\n",
    "I went to the store yesterday and bought some groceries. I got milk, bread, eggs, and cheese.\n",
    "Then I went to the hardware store to pick up some nails and a hammer. After that, I stopped\n",
    "by the pharmacy to get my prescription. It was a pretty productive day overall. I also met\n",
    "my friend Sarah at the coffee shop and we talked for about an hour about her new job.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_transcript(long_transcript, max_words=30)\n",
    "print(\"ðŸ“ Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Changes: The edited version makes the sentence more specific by providing more detailed information:\n",
      "\n",
      "*   **Person:** \"him\" was changed to the specific name \"Oscar.\"\n",
      "*   **Time:** \"yesterday\" was changed to the specific day \"on Monday.\"\n",
      "*   **Place:** \"the store\" was changed to the more specific \"the grocery store.\"\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Example 3: Explain Changes Between Versions\n",
    "\n",
    "original = \"I met him yesterday at the store.\"\n",
    "edited = \"I met oscar on Monday at the grocery store.\"\n",
    "\n",
    "explanation = explain_edits(original, edited)\n",
    "print(\"ðŸ“Š Changes:\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: um like i was saying uh the meeting was you know really good and stuff\n",
      "Improved: The meeting was really good.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Example 4: Improve Transcript Quality\n",
    "\n",
    "messy_transcript = \"um like i was saying uh the meeting was you know really good and stuff\"\n",
    "\n",
    "improved = improve_transcript(\n",
    "    messy_transcript,\n",
    "    instructions=\"Remove filler words and improve clarity while keeping it casual\"\n",
    ")\n",
    "print(\"Original:\", messy_transcript)\n",
    "print(\"Improved:\", improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Complete Workflow: Lisette-Powered\n",
    "\n",
    "Demonstrating a complete workflow using **only lisette** for all AI operations - editing, explanation, summarization, and improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Original: I met him yesterday and he told me about the project deadline.\n",
      "âœï¸  Edited: I met John last Tuesday and he told me about the project deadline.\n",
      "ðŸ“Š Changes: The changes made are:\n",
      "\n",
      "1.  **\"him\" was replaced with \"John\"**: This changes a pronoun to a specific proper noun, making the person's identity clear.\n",
      "2.  **\"yesterday\" was replaced with \"last Tuesday\"**: This changes a relative time reference to a specific day, making the timing of the meeting more precise.\n",
      "ðŸ“„ Summary: John informed me about the project deadline.\n",
      "âœ¨ Improved: I met with John last Tuesday, and he informed me of the project deadline.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Complete Workflow: Using Lisette for Everything\n",
    "\n",
    "# 1ï¸âƒ£ Start with a raw transcript\n",
    "raw_transcript = \"I met him yesterday and he told me about the project deadline.\"\n",
    "print(\"ðŸ“ Original:\", raw_transcript)\n",
    "\n",
    "# 2ï¸âƒ£ Use lisette to make edits (flexible AI-based editing)\n",
    "edit_instruction = \"Change 'him' to 'John' and 'yesterday' to 'last Tuesday'. Return only the edited text.\"\n",
    "edited = ai_chat(edit_instruction + f\"\\n\\nText: {raw_transcript}\")\n",
    "print(\"âœï¸  Edited:\", edited)\n",
    "\n",
    "# 3ï¸âƒ£ Get AI explanation of changes (lisette)\n",
    "explanation = explain_edits(raw_transcript, edited)\n",
    "print(\"ðŸ“Š Changes:\", explanation)\n",
    "\n",
    "# 4ï¸âƒ£ Generate summary (lisette)\n",
    "summary = summarize_transcript(edited, max_words=15)\n",
    "print(\"ðŸ“„ Summary:\", summary)\n",
    "\n",
    "# 5ï¸âƒ£ Improve transcript quality (lisette)\n",
    "improved = improve_transcript(edited, instructions=\"Make it more formal and professional\")\n",
    "print(\"âœ¨ Improved:\", improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Example: Sequential Editing with Lisette\n",
    "\n",
    "Demonstrates two editing steps using lisette:\n",
    "1. Replace all \"him\" with \"oscar\"\n",
    "2. Replace \"yesterday\" with \"on Monday\"\n",
    "\n",
    "Each step uses natural language instructions with lisette's flexible AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Original: I told him that I saw him yesterday. Then I asked him if he could help.\n",
      "âœï¸  After edit 1: I told oscar that I saw oscar yesterday. Then I asked oscar if he could help.\n",
      "âœï¸  After edit 2: I told oscar that I saw oscar on Monday. Then I asked oscar if he could help.\n",
      "âœ… Final transcript: I told oscar that I saw oscar on Monday. Then I asked oscar if he could help.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Example: Sequential Editing with Lisette\n",
    "\n",
    "# Initial transcript\n",
    "input_text = \"I told him that I saw him yesterday. Then I asked him if he could help.\"\n",
    "print(\"ðŸ“ Original:\", input_text)\n",
    "\n",
    "# 1ï¸âƒ£ First edit\n",
    "instruction1 = \"Change all occurrences of 'him' to 'oscar'. Return only the edited text.\"\n",
    "input_text = ai_chat(instruction1 + f\"\\n\\nText: {input_text}\")\n",
    "print(\"âœï¸  After edit 1:\", input_text)\n",
    "\n",
    "# 2ï¸âƒ£ Second edit\n",
    "instruction2 = \"Now change 'yesterday' to 'on Monday'. Return only the edited text.\"\n",
    "input_text = ai_chat(instruction2 + f\"\\n\\nText: {input_text}\")\n",
    "print(\"âœï¸  After edit 2:\", input_text)\n",
    "\n",
    "print(\"âœ… Final transcript:\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© Inspecting the Conversation\n",
    "\n",
    "Print the `_messages` list to see what the model sees on each call.\n",
    "\n",
    "**Key observations:**\n",
    "- One assistant message with the current transcript\n",
    "- Multiple user instructions recording session history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Message history:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"ðŸ§© Message history:\")\n",
    "pprint(_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "**Architecture:**\n",
    "- Hybrid context: single assistant message (current state) + cumulative user instructions (history)\n",
    "- Efficient for long transcripts with complex edit sequences\n",
    "- Supports natural, conversational editing patterns\n",
    "\n",
    "**Supported Operations:**\n",
    "1. `replace_all` â€” exact text replacement\n",
    "2. `regex_replace` â€” pattern-based with capture groups (\\1, \\2, etc.)\n",
    "3. `insert_at` â€” insert at character position (0-indexed)\n",
    "4. `insert_after` â€” insert after marker string\n",
    "5. `delete` â€” remove first or all occurrences\n",
    "\n",
    "**Future Enhancements:**\n",
    "- Token usage tracking\n",
    "- Operation history with undo/redo\n",
    "- UI integration (TUI/web)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Testing Different Edit Types with Lisette\n",
    "\n",
    "Let's test various text editing operations using lisette's natural language interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Meeting on 2025-10-07 and another on 2025-12-25.\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\\nPlease retry in 7.812891951s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.5-flash\"\n            },\n            \"quotaValue\": \"10\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"7s\"\n      }\n    ]\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:1901\u001b[0m, in \u001b[0;36mVertexLLM.completion\u001b[0;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1901\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:780\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m--> 780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:762\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    761\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m--> 762\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyAlwBC_GtjpynSnSA0kam58AJPcyMBGfDA'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mVertexAIError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/main.py:2689\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m     new_params \u001b[38;5;241m=\u001b[39m deepcopy(optional_params)\n\u001b[0;32m-> 2689\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mvertex_chat_completion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   2690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2694\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   2696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_ai_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2709\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2711\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:1905\u001b[0m, in \u001b[0;36mVertexLLM.completion\u001b[0;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[1;32m   1904\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[0;32m-> 1905\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[1;32m   1906\u001b[0m         status_code\u001b[38;5;241m=\u001b[39merror_code,\n\u001b[1;32m   1907\u001b[0m         message\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   1908\u001b[0m         headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1909\u001b[0m     )\n\u001b[1;32m   1910\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException:\n",
      "\u001b[0;31mVertexAIError\u001b[0m: {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\\nPlease retry in 7.812891951s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.5-flash\"\n            },\n            \"quotaValue\": \"10\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"7s\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use lisette to convert dates\u001b[39;00m\n\u001b[1;32m      8\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert all dates from YYYY-MM-DD format to MM/DD/YYYY format. Return only the edited text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mai_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mText: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtest_text\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[76], line 25\u001b[0m, in \u001b[0;36mai_chat\u001b[0;34m(prompt, model, enable_search)\u001b[0m\n\u001b[1;32m     23\u001b[0m search_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m enable_search \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m chat \u001b[38;5;241m=\u001b[39m Chat(model, search\u001b[38;5;241m=\u001b[39msearch_level)\n\u001b[0;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/lisette/core.py:240\u001b[0m, in \u001b[0;36mChat.__call__\u001b[0;34m(self, msg, prefill, temp, think, search, stream, max_steps, final_prompt, return_all, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream: \u001b[38;5;28;01mreturn\u001b[39;00m result_gen              \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_all: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(result_gen)  \u001b[38;5;66;03m# toolloop behavior\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_gen\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/fastcore/basics.py:732\u001b[0m, in \u001b[0;36mlast\u001b[0;34m(x, f, negate, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f: x \u001b[38;5;241m=\u001b[39m filter_ex(x, f\u001b[38;5;241m=\u001b[39mf, negate\u001b[38;5;241m=\u001b[39mnegate, gen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    731\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m x: \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/lisette/core.py:202\u001b[0m, in \u001b[0;36mChat._call\u001b[0;34m(self, msg, prefill, temp, think, search, stream, max_steps, step, final_prompt, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_search(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel) \u001b[38;5;129;01mand\u001b[39;00m (s\u001b[38;5;241m:=\u001b[39mifnone(search,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch)): kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_context_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: effort[s]}\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: _\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 202\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prep_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meffort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# temperature is not supported when reasoning\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mifnone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefill: \u001b[38;5;28;01myield\u001b[39;00m _mk_prefill(prefill)\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/utils.py:1344\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1341\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1342\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1343\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1344\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/utils.py:1219\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1217\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1222\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1223\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1224\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/main.py:3494\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   3491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   3492\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3493\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2301\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2300\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/Documents/VSCode Projects/Swiftner/tui_writer/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1266\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m429 Quota exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuota exceeded for\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[1;32m   1264\u001b[0m ):\n\u001b[1;32m   1265\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[1;32m   1267\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm.RateLimitError: VertexAIException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1268\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1269\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1270\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m   1271\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[1;32m   1272\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m429\u001b[39m,\n\u001b[1;32m   1273\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m   1274\u001b[0m                 method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1275\u001b[0m                 url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://cloud.google.com/vertex-ai/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1276\u001b[0m             ),\n\u001b[1;32m   1277\u001b[0m         ),\n\u001b[1;32m   1278\u001b[0m     )\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m500 Internal Server Error\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model is overloaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[1;32m   1282\u001b[0m ):\n\u001b[1;32m   1283\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\\nPlease retry in 7.812891951s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.5-flash\"\n            },\n            \"quotaValue\": \"10\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"7s\"\n      }\n    ]\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Test 1: Date Format Conversion\n",
    "\n",
    "test_text = \"Meeting on 2025-10-07 and another on 2025-12-25.\"\n",
    "print(f\"Original: {test_text}\")\n",
    "\n",
    "# Use lisette to convert dates\n",
    "instruction = \"Convert all dates from YYYY-MM-DD format to MM/DD/YYYY format. Return only the edited text.\"\n",
    "result = ai_chat(instruction + f\"\\n\\nText: {test_text}\")\n",
    "print(f\"Result:   {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: HelloWorld\n",
      "Result:   Hello World\n",
      "Result:   Hello World\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Test 2: Insert Space\n",
    "\n",
    "test_text = \"HelloWorld\"\n",
    "print(f\"Original: {test_text}\")\n",
    "\n",
    "instruction = \"Add a space between Hello and World. Return only the edited text.\"\n",
    "result = ai_chat(instruction + f\"\\n\\nText: {test_text}\")\n",
    "print(f\"Result:   {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, my name is John. I love coding.\n",
      "Result:   Hello, my name is John Smith. I love coding.\n",
      "Result:   Hello, my name is John Smith. I love coding.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Test 3: Insert After Marker\n",
    "\n",
    "test_text = \"Hello, my name is John. I love coding.\"\n",
    "print(f\"Original: {test_text}\")\n",
    "\n",
    "instruction = \"Add ' Smith' right after 'John'. Return only the edited text.\"\n",
    "result = ai_chat(instruction + f\"\\n\\nText: {test_text}\")\n",
    "print(f\"Result:   {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I like apples and apples are great!\n",
      "Result:   I like and apples are great!\n",
      "Result:   I like and apples are great!\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Test 4: Delete First Occurrence\n",
    "\n",
    "test_text = \"I like apples and apples are great!\"\n",
    "print(f\"Original: {test_text}\")\n",
    "\n",
    "instruction = \"Delete only the first occurrence of 'apples'. Return only the edited text.\"\n",
    "result = ai_chat(instruction + f\"\\n\\nText: {test_text}\")\n",
    "print(f\"Result:   {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I like apples and apples are great!\n",
      "Result:   I like and are great!\n",
      "Result:   I like and are great!\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Test 5: Delete All Occurrences\n",
    "\n",
    "test_text = \"I like apples and apples are great!\"\n",
    "print(f\"Original: {test_text}\")\n",
    "\n",
    "instruction = \"Delete all occurrences of 'apples'. Return only the edited text.\"\n",
    "result = ai_chat(instruction + f\"\\n\\nText: {test_text}\")\n",
    "print(f\"Result:   {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The meeting is on 2025-10-07 at the office. Please confirm.\n",
      "Result:   The meeting is on 10/07/2025 at the conference room. Please confirm. (urgent)\n",
      "Result:   The meeting is on 10/07/2025 at the conference room. Please confirm. (urgent)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "### Test 6: Complex Multi-Operation Edit\n",
    "\n",
    "test_text = \"The meeting is on 2025-10-07 at the office. Please confirm.\"\n",
    "print(f\"Original: {test_text}\")\n",
    "\n",
    "instruction = \"\"\"Make these changes:\n",
    "1. Change the date format from YYYY-MM-DD to MM/DD/YYYY\n",
    "2. Change 'office' to 'conference room'\n",
    "3. Add ' (urgent)' at the end\n",
    "\n",
    "Return only the edited text.\"\"\"\n",
    "result = ai_chat(instruction + f\"\\n\\nText: {test_text}\")\n",
    "print(f\"Result:   {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from smolagents import CodeAgent, InferenceClientModel, WebSearchTool\n",
    "#https://huggingface.co/docs/smolagents/index\n",
    "\n",
    "\n",
    "# Connect to running vLLM server using OpenAI-compatible API\n",
    "# model = OpenAIServerModel(\n",
    "#     model_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "#     api_base=\"http://localhost:8000/v1\",\n",
    "#     api_key=\"dummy\",  # vLLM doesn't require a real API key\n",
    "#     temperature=0.1,  # Lower temperature for more consistent output\n",
    "# )\n",
    "\n",
    "# model = InferenceClientModel()\n",
    "# agent = CodeAgent(\n",
    "#     tools=[WebSearchTool()],\n",
    "#     model = model \n",
    "# )\n",
    "\n",
    "# # Test with a simple question first\n",
    "# print(\"Testing simple question...\")\n",
    "# response = agent.run(\"What is the square root of 75?\")\n",
    "# print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
