{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ba62da",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: ai.html\n",
    "title: AI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ba8ea",
   "metadata": {},
   "source": [
    "## AI Module\n",
    "This module implements the core AI-powered transcription editing system. It distinguishes between regular speech transcription and natural language edit commands, enabling a conversational editing workflow.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The system uses a Chat-based LLM (via lisette) that: <br>\n",
    "- Maintains conversation history to understand context <br>\n",
    "- Distinguishes between new content and edit instructions <br>\n",
    "- Returns either \"APPEND\" for new text or the complete edited transcript <br>\n",
    "- Tracks token usage across the entire session <br>\n",
    "\n",
    "### Multi-Provider Support\n",
    "\n",
    "Works with multiple LLM providers through lisette: <br>\n",
    "- **OpenAI**: GPT-4o, GPT-4o-mini, GPT-3.5-turbo <br>\n",
    "- **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus/Sonnet/Haiku <br>\n",
    "- **Google**: Gemini 1.5 Pro/Flash <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc4e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc7103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "from lisette import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d16540",
   "metadata": {},
   "source": [
    "## TranscriptEditor Class\n",
    "\n",
    "The `TranscriptEditor` class manages live transcription with AI-assisted editing capabilities. It uses a Chat instance that maintains conversation history, allowing it to understand context across multiple chunks of transcription.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "```python\n",
    "editor = TranscriptEditor(model=\"openai/gpt-4o-mini\", temperature=0.1)\n",
    "```\n",
    "\n",
    "**Parameters:** <br>\n",
    "- `model`: Full model identifier in format \"provider/model-name\" (e.g., \"openai/gpt-4o-mini\") <br>\n",
    "\n",
    "### System Prompt Strategy\n",
    "\n",
    "The editor uses a carefully crafted system prompt that instructs the LLM to: <br>\n",
    "1. Detect edit commands in natural language <br>\n",
    "2. Return \"APPEND\" when the user is adding new content <br>\n",
    "3. Return the full corrected transcript when an edit is requested <br>\n",
    "4. Maintain context through conversation history <br>\n",
    "\n",
    "This approach allows the LLM to understand references like \"that\", \"the last part\", or \"the first sentence\" because it has access to the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95492f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TranscriptEditor:\n",
    "    \"\"\"Manages live transcription with AI-assisted editing capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, temperature: float = 0.1):\n",
    "        self.chat = Chat(\n",
    "            model,\n",
    "            sp=\"\"\"You are helping with live transcription. As the user speaks, you'll receive each transcribed chunk (Each chunk being a line of text).\n",
    "Your job is to:\n",
    "1. Detect when the user wants to edit previous text (e.g., \"change that to...\", \"delete the last part\", \"replace hamburgers with pizza\")\n",
    "2. When an edit is requested, return ONLY the complete corrected/edited transcript\n",
    "3. When it's just new text, return ONLY the word \"APPEND\"\n",
    "4. Keep the conversation history to understand context\n",
    "\n",
    "Format your responses as:\n",
    "- For edits: Return the full corrected transcript, each sentence on its own line\n",
    "- For new text: APPEND\"\"\",\n",
    "            temp=temperature\n",
    "        )\n",
    "        self.full_transcript = \"\"\n",
    "        self.total_tokens = 0\n",
    "    \n",
    "    def process_chunk(self, chunk: str) -> dict:\n",
    "        \"\"\"Process a transcription chunk and determine if it's new text or an edit.\"\"\"\n",
    "        \n",
    "        # Send the chunk with context about current transcript\n",
    "        response = self.chat(chunk)\n",
    "        \n",
    "        result = response.choices[0].message.content.strip()\n",
    "        tokens = response.usage.total_tokens if hasattr(response, \"usage\") else 0\n",
    "        self.total_tokens += tokens\n",
    "        \n",
    "        # Determine if it's an append or edit\n",
    "        if result.startswith(\"APPEND\"):\n",
    "            self.full_transcript += result\n",
    "            action = \"append\"\n",
    "        else:\n",
    "            # It's an edit - replace full transcript\n",
    "            self.full_transcript = result\n",
    "            action = \"edit\"\n",
    "        \n",
    "        return {\n",
    "            \"transcript\": self.full_transcript,\n",
    "            \"action\": action,\n",
    "            \"tokens_used\": tokens,\n",
    "            \"total_tokens\": self.total_tokens\n",
    "        }\n",
    "    \n",
    "    def get_transcript(self) -> str:\n",
    "        \"\"\"Get the current full transcript.\"\"\"\n",
    "        return self.full_transcript\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the transcript and chat history.\"\"\"\n",
    "        self.full_transcript = \"\"\n",
    "        self.total_tokens = 0\n",
    "        self.chat = Chat(\n",
    "            self.chat.model,\n",
    "            sp=self.chat.sp,\n",
    "            temp=self.chat.temp\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe1eb4",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### Process Flow\n",
    "\n",
    "1. **User speaks**: \"I love pizza\" <br>\n",
    "2. **Transcriber produces**: \"I love pizza.\" <br>\n",
    "3. **AI processes**: Detects new content → Returns \"APPEND\" <br>\n",
    "4. **Transcript updated**: Appends the new text <br>\n",
    "\n",
    "Then later: <br>\n",
    "\n",
    "1. **User speaks**: \"Actually, change pizza to hamburgers\" <br>\n",
    "2. **Transcriber produces**: \"Actually, change pizza to hamburgers.\" <br>\n",
    "3. **AI processes**: Detects edit command → Returns full edited transcript <br>\n",
    "4. **Transcript updated**: Replaces entire transcript with edited version <br>\n",
    "\n",
    "### Context Awareness\n",
    "\n",
    "The Chat instance maintains `hist` (history) of all previous interactions. This allows it to: <br>\n",
    "- Understand references to \"that\", \"it\", \"the last part\" <br>\n",
    "- Track what has been said throughout the session <br>\n",
    "- Make intelligent decisions about edit scope <br>\n",
    "\n",
    "### Token Management\n",
    "\n",
    "The editor tracks token usage across all API calls: <br>\n",
    "- `tokens_used`: Tokens consumed by the current chunk <br>\n",
    "- `total_tokens`: Cumulative tokens across the entire session <br>\n",
    "\n",
    "This helps users monitor API costs and understand the computational overhead of AI-assisted editing.\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "If the AI processing fails, the system gracefully degrades to simple append mode, ensuring transcription continues even if the AI service is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk: My name is Batman.\n",
      "\n",
      "Action: append\n",
      "Current transcript:\n",
      "\n",
      "Tokens used: 155\n",
      "\n",
      "--- Chunk: I love pizza.\n",
      "\n",
      "Action: append\n",
      "Current transcript:\n",
      "\n",
      "Tokens used: 169\n",
      "\n",
      "--- Chunk: This transcriber is working quite well.\n",
      "Action: append\n",
      "Current transcript:\n",
      "\n",
      "Tokens used: 187\n",
      "\n",
      "--- Chunk: Actually, change pizza to hamburgers.\n",
      "Action: edit\n",
      "Current transcript:\n",
      "My name is Batman.  \n",
      "I love hamburgers.  \n",
      "This transcriber is working quite well.\n",
      "Tokens used: 223\n",
      "\n",
      "--- Chunk: Maybe even delete that first sentence about my name.\n",
      "Action: edit\n",
      "Current transcript:\n",
      "I love hamburgers.  \n",
      "This transcriber is working quite well.\n",
      "Tokens used: 255\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Test the TranscriptEditor\n",
    "editor = TranscriptEditor(\"openai/gpt-4o-mini\")\n",
    "\n",
    "# Simulate transcription chunks\n",
    "chunks = [\n",
    "    \"My name is Batman.\\n\",\n",
    "    \"I love pizza.\\n\",\n",
    "    \"This transcriber is working quite well.\",\n",
    "    \"Actually, change pizza to hamburgers.\",\n",
    "    \"Maybe even delete that first sentence about my name.\"\n",
    "]\n",
    "\n",
    "for chunk in chunks:\n",
    "    result = editor.process_chunk(chunk)\n",
    "    print(f\"\\n--- Chunk: {chunk}\")\n",
    "    print(f\"Action: {result['action']}\")\n",
    "    print(f\"Current transcript:\\n{result['transcript']}\")\n",
    "    print(f\"Tokens used: {result['tokens_used']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
