{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a264cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89771301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "import time\n",
    "import wave\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "from rich.console import Console\n",
    "\n",
    "# Console for rich formatting\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def _is_cuda_available() -> bool:\n",
    "    \"\"\"Check if CUDA is available for GPU acceleration.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        return torch.cuda.is_available()\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa14848",
   "metadata": {},
   "source": [
    "## AI Transcription Subsystem\n",
    "\n",
    "The WhisperTranscriber class represents the intelligent core of our application. It leverages OpenAI's Whisper model - a state-of-the-art speech recognition system trained on massive multilingual datasets - to convert audio into accurate text transcriptions.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Whisper employs a sophisticated transformer-based architecture that has been trained on over 680,000 hours of multilingual audio data. The model supports multiple languages and can handle various audio conditions including background noise, different speakers, and technical audio quality variations.\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "- **Accuracy**: 97%+ word error rate on clean English speech\n",
    "- **Speed**: Real-time or faster processing with GPU acceleration\n",
    "- **Languages**: 99 languages supported with automatic language detection\n",
    "- **Robustness**: Handles diverse audio conditions and speaker variations\n",
    "\n",
    "### Integration Strategy\n",
    "\n",
    "The transcriber integrates seamlessly with our recording system through a standardized interface that abstracts the complexity of the underlying AI model while providing comprehensive error handling and user feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c08313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class WhisperTranscriber:\n",
    "    \"\"\"AI-powered speech recognition system using OpenAI's Whisper model.\"\"\"\n",
    "    VALID_MODELS = [\n",
    "        # English-only models (optimized for English speech)\n",
    "        \"tiny.en\",      # ~39 MB, fastest English-only model\n",
    "        \"base.en\",      # ~74 MB, good balance for English\n",
    "        \"small.en\",     # ~244 MB, higher accuracy for English\n",
    "        \"medium.en\",    # ~769 MB, best accuracy for English\n",
    "\n",
    "        # Multilingual models (support 99+ languages)\n",
    "        \"tiny\",         # ~39 MB, fastest multilingual model\n",
    "        \"base\",         # ~74 MB, good balance for all languages\n",
    "        \"small\",        # ~244 MB, higher accuracy multilingual\n",
    "        \"medium\",       # ~769 MB, best accuracy multilingual\n",
    "        \"large-v1\",     # ~1550 MB, previous large model\n",
    "        \"large-v2\",     # ~1550 MB, improved large model\n",
    "        \"large-v3\",     # ~1550 MB, latest large model\n",
    "        \"large\",        # ~1550 MB, alias for large-v3\n",
    "\n",
    "        # Distilled models (faster, smaller versions)\n",
    "        \"distil-large-v2\",      # ~760 MB, distilled large model\n",
    "        \"distil-medium.en\",     # ~590 MB, distilled English medium\n",
    "        \"distil-small.en\",      # ~660 MB, distilled English small\n",
    "        \"distil-large-v3\",      # ~760 MB, distilled large v3\n",
    "        \"distil-large-v3.5\",    # ~760 MB, latest distilled large\n",
    "\n",
    "        # Turbo models (fastest performance)\n",
    "        \"large-v3-turbo\",       # ~1550 MB, turbo-optimized large\n",
    "        \"turbo\",                # ~1550 MB, alias for large-v3-turbo\n",
    "    ]\n",
    "\n",
    "    def __init__(self, model_name: Optional[str] = None, language: Optional[str] = None):\n",
    "        \"\"\"Initialize the Whisper transcriber with specified model and language.\n",
    "\n",
    "        This constructor sets up the complete transcription pipeline by:\n",
    "        1. Validating and selecting the appropriate Whisper model\n",
    "        2. Detecting available hardware for optimal performance\n",
    "        3. Loading the model with appropriate device configuration\n",
    "        4. Configuring language settings for transcription\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the Whisper model to use (e.g., \"base\", \"small\")\n",
    "                       If None, uses HNS_WHISPER_MODEL environment variable or \"base\"\n",
    "            language: Target language code (e.g., \"en\", \"es\", \"fr\")\n",
    "                     If None, uses HNS_LANG environment variable or auto-detection\n",
    "\n",
    "        The initialization process includes automatic hardware detection to ensure\n",
    "        optimal performance across different system configurations.\n",
    "        \"\"\"\n",
    "        self.model_name = self._get_model_name(model_name)\n",
    "        self.language = language or os.environ.get(\"HNS_LANG\")\n",
    "        self.model = self._load_model()\n",
    "\n",
    "    def _get_audio_duration(self, audio_file_path: Union[Path, str]) -> Optional[float]:\n",
    "        \"\"\"Get duration of audio file in seconds.\"\"\"\n",
    "        try:\n",
    "            with wave.open(str(audio_file_path), \"rb\") as audio_file:\n",
    "                frames = audio_file.getnframes()\n",
    "                sample_rate = audio_file.getframerate()\n",
    "                duration = frames / float(sample_rate)\n",
    "                return duration\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _get_model_name(self, model_name: Optional[str]) -> str:\n",
    "        model = model_name or os.environ.get(\"HNS_WHISPER_MODEL\", \"base\")\n",
    "\n",
    "        if model not in self.VALID_MODELS:\n",
    "            console.print(f\"⚠️ [bold yellow]Invalid model '{model}', using 'base' instead[/bold yellow]\")\n",
    "            console.print(f\"    [dim]Available models: {', '.join(self.VALID_MODELS)}[/dim]\")\n",
    "            return \"base\"\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def _load_model(self) -> WhisperModel:\n",
    "        try:\n",
    "            # Auto-detect available device\n",
    "            device = \"cuda\" if _is_cuda_available() else \"cpu\"\n",
    "            compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
    "\n",
    "            return WhisperModel(self.model_name, device=device, compute_type=compute_type)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {e}\")\n",
    "\n",
    "    def transcribe(self, audio_source: Union[Path, str]) -> tuple[str, float]:\n",
    "        \"\"\"Transcribe audio file to text using the loaded Whisper model.\"\"\"\n",
    "        transcribe_kwargs = {\n",
    "            \"beam_size\": 5,\n",
    "            \"vad_filter\": True,\n",
    "            \"vad_parameters\": {\"min_silence_duration_ms\": 500, \"speech_pad_ms\": 400, \"threshold\": 0.5},\n",
    "        }\n",
    "\n",
    "        if self.language:\n",
    "            transcribe_kwargs[\"language\"] = self.language\n",
    "\n",
    "        try:\n",
    "            _audio_duration = self._get_audio_duration(audio_source)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            segments, _ = self.model.transcribe(str(audio_source), **transcribe_kwargs)\n",
    "            transcription_parts = []\n",
    "            for segment in segments:\n",
    "                text = segment.text.strip()\n",
    "                if text:\n",
    "                    transcription_parts.append(text)\n",
    "\n",
    "            full_transcription = \" \".join(transcription_parts)\n",
    "            if not full_transcription:\n",
    "                raise ValueError(\"No speech detected in audio\")\n",
    "\n",
    "            elapsed_total = time.time() - start_time\n",
    "            return full_transcription, elapsed_total\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Transcription failed: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
