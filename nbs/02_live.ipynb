{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| default_exp ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from fastapi import FastAPI\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.responses import StreamingResponse, HTMLResponse\n",
    "from fastrtc import (\n",
    "    AdditionalOutputs,\n",
    "    ReplyOnPause,\n",
    "    Stream,\n",
    "    AlgoOptions,\n",
    "    SileroVadOptions,\n",
    "    audio_to_bytes,\n",
    ")\n",
    "import torch\n",
    "import subprocess\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "def get_device(force_cpu=False):\n",
    "    if force_cpu:\n",
    "        return \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "    \n",
    "def get_torch_and_np_dtypes(device, use_bfloat16=False):\n",
    "    if device == \"cuda\":\n",
    "        torch_dtype = torch.bfloat16 if use_bfloat16 else torch.float16\n",
    "        np_dtype = np.float16\n",
    "    elif device == \"mps\":\n",
    "        torch_dtype = torch.bfloat16 if use_bfloat16 else torch.float16\n",
    "        np_dtype = np.float16\n",
    "    else:\n",
    "        torch_dtype = torch.float32\n",
    "        np_dtype = np.float32\n",
    "    return torch_dtype, np_dtype\n",
    "\n",
    "def cuda_version_check():\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            cuda_runtime = subprocess.check_output([\"nvcc\", \"--version\"]).decode()\n",
    "            cuda_version = cuda_runtime.split()[-2]\n",
    "        except Exception:\n",
    "            # Fallback to PyTorch's built-in version if nvcc isn't available\n",
    "            cuda_version = torch.version.cuda\n",
    "        \n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        return cuda_version, device_name\n",
    "    else:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_dotenv()\n",
    " \n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "UI_MODE = os.getenv(\"UI_MODE\", \"fastapi\").lower() # gradio | fastapi\n",
    "UI_TYPE = os.getenv(\"UI_TYPE\", \"base\").lower() # base | screen\n",
    "APP_MODE = os.getenv(\"APP_MODE\", \"local\").lower() # local | deployed\n",
    "# TURN_PROVIDER = os.getenv(\"TURN_PROVIDER\", \"hf-cloudflare\") # Not needed for local development\n",
    "\n",
    "MODEL_ID = os.getenv(\"MODEL_ID\", \"openai/whisper-base\")  # Use base model for faster processing\n",
    "LANGUAGE = os.getenv(\"LANGUAGE\", \"english\")\n",
    "\n",
    "logger.info(f\"\"\"\n",
    "    --------------------------------------\n",
    "    Configuration (environment variables):\n",
    "    - UI_MODE: {UI_MODE}\n",
    "    - UI_TYPE: {UI_TYPE}\n",
    "    - APP_MODE: {APP_MODE}\n",
    "\n",
    "    - MODEL_ID: {MODEL_ID}\n",
    "    - LANGUAGE: {LANGUAGE}\n",
    "    --------------------------------------\n",
    "\"\"\")\n",
    "\n",
    "# Initialize faster_whisper model for better performance\n",
    "device = get_device(force_cpu=False)\n",
    "transcribe_model = WhisperModel(\n",
    "    MODEL_ID,\n",
    "    device=device,\n",
    "    compute_type=\"int8\" if device == \"cpu\" else \"float16\",  # Use quantized model for CPU, float16 for GPU\n",
    "    download_root=None,\n",
    "    local_files_only=False\n",
    ")\n",
    "\n",
    "async def transcribe(audio: tuple[int, np.ndarray]):\n",
    "    sample_rate, audio_array = audio\n",
    "    logger.info(f\"Sample rate: {sample_rate}Hz, Shape: {audio_array.shape}\")\n",
    "\n",
    "    # Convert audio bytes to the format expected by faster_whisper\n",
    "    audio_bytes = audio_to_bytes(audio)\n",
    "\n",
    "    # Use faster_whisper for transcription\n",
    "    segments, info = transcribe_model.transcribe(\n",
    "        audio_bytes,\n",
    "        language=LANGUAGE,\n",
    "        beam_size=1,  # Faster with beam_size=1\n",
    "        condition_on_previous_text=False,  # Faster for real-time\n",
    "    )\n",
    "\n",
    "    # Collect all segments into text\n",
    "    transcript_text = \" \".join([segment.text.strip() for segment in segments])\n",
    "    yield AdditionalOutputs(transcript_text.strip())\n",
    "\n",
    "\n",
    "logger.info(\"Initializing FastRTC stream\")\n",
    "stream = Stream(\n",
    "    handler=ReplyOnPause(\n",
    "        transcribe,\n",
    "        algo_options=AlgoOptions(\n",
    "            # Duration in seconds of audio chunks passed to the VAD model (default 0.6)\n",
    "            audio_chunk_duration=0.3,  # Reduced for faster response\n",
    "            # If the chunk has more than started_talking_threshold seconds of speech, the user started talking (default 0.2)\n",
    "            started_talking_threshold=0.05,  # Lower threshold for faster detection\n",
    "            # If, after the user started speaking, there is a chunk with less than speech_threshold seconds of speech, the user stopped speaking. (default 0.1)\n",
    "            speech_threshold=0.05,  # Lower threshold for faster detection\n",
    "            # Max duration of speech chunks before the handler is triggered, even if a pause is not detected by the VAD model. (default -inf)\n",
    "            max_continuous_speech_s=10  # Reduced for faster processing\n",
    "        ),\n",
    "        model_options=SileroVadOptions(\n",
    "            # Threshold for what is considered speech (default 0.5)\n",
    "            threshold=0.4,  # Lower threshold for better detection\n",
    "            # Final speech chunks shorter min_speech_duration_ms are thrown out (default 250)\n",
    "            min_speech_duration_ms=150,  # Reduced for faster processing\n",
    "            # Max duration of speech chunks, longer will be split at the timestamp of the last silence that lasts more than 100ms (if any) or just before max_speech_duration_s (default float('inf')) (used internally in the VAD algorithm to split the audio that's passed to the algorithm)\n",
    "            max_speech_duration_s=8,  # Reduced for faster processing\n",
    "            # Wait for ms at the end of each speech chunk before separating it (default 2000)\n",
    "            min_silence_duration_ms=200,  # Reduced for faster response\n",
    "            # Chunk size for VAD model. Can be 512, 1024, 1536 for 16k s.r. (default 1024)\n",
    "            window_size_samples=512,  # Smaller window for faster processing\n",
    "            # Final speech chunks are padded by speech_pad_ms each side (default 400)\n",
    "            speech_pad_ms=100,  # Reduced padding\n",
    "        ),\n",
    "    ),\n",
    "    # send-receive: bidirectional streaming (default)\n",
    "    # send: client to server only\n",
    "    # receive: server to client only\n",
    "    modality=\"audio\",\n",
    "    mode=\"send\",\n",
    "    additional_outputs=[\n",
    "        gr.Textbox(label=\"Transcript\"),\n",
    "    ],\n",
    "    additional_outputs_handler=lambda current, new: current + \" \" + new,\n",
    "    rtc_configuration=None,  # Not needed for local development\n",
    ")\n",
    "\n",
    "app = FastAPI()\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "stream.mount(app)\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def index():\n",
    "    if UI_TYPE == \"base\":\n",
    "        html_content = open(\"static/index.html\").read()\n",
    "    elif UI_TYPE == \"screen\":\n",
    "        html_content = open(\"static/index-screen.html\").read()\n",
    "\n",
    "    rtc_configuration = None  # Not needed for local development\n",
    "    logger.info(f\"RTC configuration: {rtc_configuration}\")\n",
    "    html_content = html_content.replace(\"__INJECTED_RTC_CONFIG__\", json.dumps(rtc_configuration))\n",
    "    return HTMLResponse(content=html_content)\n",
    "\n",
    "@app.get(\"/transcript\")\n",
    "def _(webrtc_id: str):\n",
    "    logger.debug(f\"New transcript stream request for webrtc_id: {webrtc_id}\")\n",
    "    async def output_stream():\n",
    "        try:\n",
    "            async for output in stream.output_stream(webrtc_id):\n",
    "                transcript = output.args[0]\n",
    "                logger.debug(f\"Sending transcript for {webrtc_id}: {transcript[:50]}...\")\n",
    "                yield f\"event: output\\ndata: {transcript}\\n\\n\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in transcript stream for {webrtc_id}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    return StreamingResponse(output_stream(), media_type=\"text/event-stream\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    server_name = os.getenv(\"SERVER_NAME\", \"localhost\")\n",
    "    port = os.getenv(\"PORT\", 7860)\n",
    "    \n",
    "    if UI_MODE == \"gradio\":\n",
    "        logger.info(\"Launching Gradio UI\")\n",
    "        stream.ui.launch(\n",
    "            server_port=port, \n",
    "            server_name=server_name,\n",
    "            ssl_verify=False,\n",
    "            debug=True\n",
    "        )\n",
    "    else:\n",
    "        import uvicorn\n",
    "        logger.info(\"Launching FastAPI server\")\n",
    "        uvicorn.run(app, host=server_name, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
