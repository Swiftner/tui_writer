# Transcription


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Live Transcription

This module implements a real-time speech-to-text system that: <br> -
Listens to microphone input continuously <br> - Uses **Silero VAD**
(Voice Activity Detection) to detect when you are speaking <br> -
Automatically segments audio into utterances based on natural pauses
<br> - Transcribes each utterance using **Faster-Whisper** (optimized
Whisper implementation) <br> - Streams transcribed text chunks as they
become available <br>

The system is optimized for interactive applications that need
responsive, chunked speech transcription with minimal latency.

### Architecture Overview

The implementation uses a multi-threaded, asynchronous architecture:

1.  **PyAudio Thread**: Captures raw audio from the microphone <br>
2.  **AsyncIO Queue**: Thread-safe bridge between audio callback and
    async processing <br>
3.  **Processing Loop**: Analyzes audio with VAD and triggers
    transcription <br>
4.  **Worker Threads**: Runs Whisper transcription without blocking the
    main loop <br>

This design ensures audio capture never blocks or drops frames while
maintaining responsive transcription.

## Imports and Device Selection

The module imports only essential dependencies: <br> - **numpy**: Audio
data manipulation and buffer management <br> - **torch**: Required for
Silero VAD model <br> - **pyaudio**: Cross-platform audio I/O <br> -
**faster_whisper**: Optimized Whisper implementation with CTranslate2
backend <br> - **asyncio**: Non-blocking audio processing <br> -
**logging**: Debug and status output <br>

### Device Selection Strategy

The `get_device()` helper automatically selects the best available
compute device:

1.  **CPU (forced)**: If `force_cpu=True` <br>
2.  **CUDA**: NVIDIA GPU acceleration (if available) <br>
3.  **MPS**: Apple Silicon Metal Performance Shaders (if available) <br>
4.  **CPU (fallback)**: Default fallback <br>

For Whisper models, device selection impacts speed significantly: <br> -
CPU: Slower but works everywhere <br> - CUDA: 5-10x faster on compatible
NVIDIA GPUs <br> - MPS: 3-5x faster on M1/M2/M3 Macs <br>

## üó£Ô∏è Silero VAD (Voice Activity Detection)

This small helper loads the **Silero VAD** model from `torch.hub`.

Silero VAD is a lightweight neural model that outputs the **probability
of speech** (0‚Äì1).  
We‚Äôll use it to decide when the user is talking or has paused ‚Äî so we
can send only meaningful speech chunks to Whisper.

If Silero fails to load (e.g., offline), we just return `None` and
handle it later.

# üéôÔ∏è LiveTranscriber ‚Äî Real-time Speech-to-Text

The `LiveTranscriber` class captures live audio from your microphone,
detects when you‚Äôre speaking using **Silero VAD**, and transcribes each
spoken sentence using **Faster-Whisper** once you pause.

It runs asynchronously, making it ideal for real-time interfaces like
TUIs or assistants.

------------------------------------------------------------------------

### ‚öôÔ∏è Parameters

- **`model_id`** ‚Äì Whisper model to use (e.g.¬†`"tiny"`, `"base"`,
  `"small"`).  
- **`language`** ‚Äì Language code for transcription (default `"en"`).  
- **`force_cpu`** ‚Äì Force CPU usage even if GPU is available.  
- **`on_transcript`** ‚Äì Callback called with each transcribed text
  chunk.  
- **`vad_threshold`** ‚Äì Silero confidence threshold (0.0‚Äì1.0, higher =
  stricter).  
- **`min_speech_duration_ms`** ‚Äì Minimum length of speech to count as
  valid.  
- **`min_silence_duration_ms`** ‚Äì How long silence must last before
  starting transcription.

------------------------------------------------------------------------

### üß† Main Methods

- **`start()`** ‚Äî Begins microphone capture and transcription loop
  (async).  
- **`stop()`** ‚Äî Gracefully stops audio processing.  
- **`process_audio()`** ‚Äî Runs continuously, detecting speech/silence
  and triggering transcription.  
- **`_transcribe_chunk()`** ‚Äî Uses Whisper to transcribe one full
  utterance.  
- **`_detect_speech_silero()`** ‚Äî Returns `True` if Silero VAD detects
  speech in the current chunk.

------------------------------------------------------------------------

### üîÑ Summary

- Audio is streamed in 32 ms chunks (512 samples at 16 kHz).  
- Each chunk is passed to Silero VAD ‚Üí speech or silence.  
- When silence lasts long enough, the buffered audio is sent to
  Whisper.  
- The result is sent to your `on_transcript` callback.

------------------------------------------------------------------------

### LiveTranscriber

>  LiveTranscriber (model_id:str='openai/whisper-base', language:str='en',
>                       force_cpu:bool=False,
>                       on_transcript:Optional[Callable[[str],NoneType]]=None,
>                       vad_threshold:float=0.5, min_speech_duration_ms:int=250,
>                       min_silence_duration_ms:int=500)

*Improved LiveTranscriber: - uses asyncio.Queue for clean async
consumption - pushes audio from audio thread with
loop.call_soon_threadsafe - collects chunks in a list (no repeated
np.append) - does NOT append silence chunks into the sent buffer -
dispatches transcription to a thread and safely calls callbacks -
provides stop() for clean shutdown*

## üß™ Live Transcription Test

This test starts a 10-second live recording session using the
`LiveTranscriber`.  
Speak naturally in short sentences ‚Äî each pause will automatically
trigger a transcription.  
Each transcribed chunk is printed as soon as it‚Äôs ready, and all results
are shown at the end.

``` python
import asyncio

all_chunks = []

def handle_transcript_chunk(text: str):
    """Callback called whenever a transcription chunk is ready."""
    if text.strip():
        print(f"\n[TRANSCRIBED] {text}")
        all_chunks.append(text)

async def test_live_transcription(duration_seconds: int = 10):
    all_chunks.clear()
    print("üé§ Speak in short sentences; pauses will trigger transcription.")
    transcriber = LiveTranscriber(
        model_id="base",          # keep whatever model id you use
        language="en",
        on_transcript=handle_transcript_chunk,
        vad_threshold=0.5,
        min_speech_duration_ms=250,
        min_silence_duration_ms=500,
    )

    # start the transcriber in the background
    start_task = asyncio.create_task(transcriber.start())

    try:
        # run for given duration (you can interrupt with Ctrl+C)
        await asyncio.sleep(duration_seconds)
    except KeyboardInterrupt:
        print("Interrupted by user.")
    finally:
        # ask the transcriber to stop and wait for it to finish
        await transcriber.stop()

        # wait for the start() task to exit cleanly
        try:
            await start_task
        except Exception as e:
            # if any error bubbled up from start/process_audio, show it
            print(f"Transcriber task ended with exception: {e}")

    print("\nüìù Full transcript:")
    for i, t in enumerate(all_chunks, 1):
        print(f"{i}. {t}")

# run it
await test_live_transcription(10)
```

    üé§ Speak in short sentences; pauses will trigger transcription.

    Using cache found in /home/jens/.cache/torch/hub/snakers4_silero-vad_master
    ALSA lib pcm_dsnoop.c:567:(snd_pcm_dsnoop_open) unable to open slave
    ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave
    ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear
    ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe
    ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side
    ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave


    [TRANSCRIBED] Okay, can you hear me?

    [TRANSCRIBED] That's nice.

    [TRANSCRIBED] What if now?

    [TRANSCRIBED] and now

    üìù Full transcript:
    1. Okay, can you hear me?
    2. That's nice.
    3. What if now?
    4. and now
