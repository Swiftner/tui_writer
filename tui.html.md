# TUI


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## TUI Module - Terminal User Interface

This module implements the complete Textual-based terminal user
interface for live audio transcription with AI-powered editing
capabilities.

### Overview

The TUI provides a full-featured interactive interface that combines:
<br> - Real-time speech transcription with Faster Whisper <br> -
AI-powered edit detection and application <br> - Multi-provider AI
support (OpenAI, Anthropic, Google) <br> - Keyboard-driven workflow with
modal dialogs <br> - Visual feedback for recording state and AI
operations <br>

### Architecture

The interface is built with Textual, a modern Python framework for
building terminal UIs. The architecture consists of:

1.  **Main Application** (`TranscriptionTUI`): Coordinates all
    components and manages state <br>
2.  **Modal Screens**: Settings dialogs for audio and AI configuration
    <br>
3.  **Help System**: Comprehensive in-app documentation <br>
4.  **Event Handlers**: Keyboard shortcuts and user interactions <br>
5.  **Reactive UI**: Automatic updates based on state changes <br>

### Key Features

- **Live Transcription Display**: Scrolling log that shows transcripts
  in real-time <br>
- **Recording Indicator**: Header turns red during active recording <br>
- **Status Notifications**: Toast-style messages for user feedback <br>
- **Clipboard Integration**: One-key copy of entire transcript <br>
- **Persistent Configuration**: Remembers settings and API keys between
  sessions <br>

## Styling with Textual CSS

The CSS defines the visual appearance of the TUI using Textual’s
CSS-like syntax. Key features: <br> - **Recording indicator**: Changes
header background to red (`$error`) when recording <br> - **Transcript
display**: Styled with hatching pattern and padding for readability
<br> - **Modal layouts**: Centered dialogs for settings and help screens
<br> - Uses Textual’s built-in color variables like `$background`,
`$surface`, `$boost`, etc. <br>

## SettingsModal - Audio Configuration

Modal dialog for configuring Whisper transcription settings. Accessed by
pressing `s` in the main interface.

### Available Models

The modal offers 10 Whisper model options with trade-offs between speed
and accuracy:

**Standard Models** (multilingual): <br> - **Tiny**: Ultra fast, low
accuracy - Good for testing or real-time needs <br> - **Base**: Fast,
decent accuracy - Recommended starting point <br> - **Small**: Balanced
speed and accuracy - Good general purpose choice <br> - **Medium**:
Slow, high accuracy - For when quality matters <br> - **Large-v3**: Very
slow, best accuracy - Maximum quality <br>

**English-Only Models** (faster variants): <br> - **Tiny.en** through
**Large.en**: Optimized for English, typically 2x faster than
multilingual equivalents <br>

### Language Support

Supports 40+ languages including: <br> - European: English, German,
French, Spanish, Italian, Portuguese, Dutch, Polish, Czech, Slovak, etc.
<br> - Nordic: Norwegian, Swedish, Danish, Finnish <br> - Slavic:
Russian, Ukrainian, Serbian, Croatian, Bulgarian, Slovenian <br> -
Asian: Chinese (Mandarin/Cantonese), Japanese, Korean, Hindi, Bengali,
Thai, Vietnamese, Indonesian <br> - Middle Eastern: Arabic, Hebrew,
Persian, Turkish, Urdu <br> - African: Swahili, Afrikaans <br>

### Implementation

Uses Textual’s `Select` widgets for dropdown menus. Selected values are
returned when the modal is dismissed and applied via the
`apply_settings()` callback in the main app.

## AI Settings Modal

Advanced modal for managing AI models across different providers.
Accessed by pressing `a` in the main interface.

### Multi-Provider Architecture

The modal provides a unified interface for three LLM providers:

**OpenAI** - Most popular, wide range of models: <br> - GPT-4o: Latest
flagship model, fast and capable <br> - GPT-4o Mini: Smaller, faster,
cost-effective variant <br> - GPT-4 Turbo: Previous generation flagship
<br> - GPT-4: Original GPT-4 <br> - GPT-3.5 Turbo: Fast and economical
<br>

**Anthropic** - Claude models known for safety and helpfulness: <br> -
Claude 3.5 Sonnet: Latest and most capable <br> - Claude 3 Opus:
Powerful reasoning and analysis <br> - Claude 3 Sonnet: Balanced
performance <br> - Claude 3 Haiku: Fast and efficient <br>

**Google Gemini** - Google’s multimodal models: <br> - Gemini 1.5 Pro:
Large context window, powerful <br> - Gemini 1.5 Flash: Fast inference,
economical <br> - Gemini 1.0 Pro: Previous generation <br>

### Dynamic UI Updates

When you change providers, the modal: <br> 1. Updates the model dropdown
with provider-specific options <br> 2. Loads any existing API key for
that provider (displayed as masked dots) <br> 3. Updates the placeholder
text in the API key field <br> 4. Preserves the last selected model if
switching back to a previous provider <br>

### API Key Management

API keys are: <br> - Stored in XDG-compliant config file at
`~/.config/tui_writer/tui_writer.conf` <br> - Displayed as dots for
security when present <br> - Only updated if you type a new value
(leaving blank keeps existing key) <br> - Loaded into environment
variables (`OPENAI_API_KEY`, etc.) when the app starts <br>

### Persistence

The modal remembers: <br> - Last used provider <br> - Last selected
model for each provider <br> - API keys for all providers <br>

This allows seamless switching between providers without re-entering
credentials.

## HelpModal

Interactive help screen accessible by pressing `?` in the main
interface. Displays comprehensive documentation using Textual’s
MarkdownViewer component.

### Content Structure

The help documentation covers:

**Usage Instructions:** <br> - How to start and stop recording <br> -
How the AI automatically detects edit commands <br> - Example voice
commands for editing <br>

**Settings Configuration:** <br> - Audio settings: Model and language
selection <br> - AI settings: Provider and model configuration <br>

**Keyboard Shortcuts:** <br> - Complete reference table of all available
shortcuts <br> - Quick access to common operations <br>

### Features

- **Markdown Rendering**: Full support for headings, lists, tables, and
  formatting <br>
- **Table of Contents**: Automatic navigation sidebar for long documents
  <br>
- **Scrollable**: Can handle extensive documentation <br>
- **Keyboard Dismissal**: Press Esc or click Close button to exit <br>

### Implementation

The `HELP_MARKDOWN` constant contains the documentation as a multi-line
string. The MarkdownViewer widget handles parsing and rendering with
syntax highlighting and proper formatting.

This approach allows easy updates to documentation without changing code
structure - just edit the markdown content.

## Transcription TUI

The core application that integrates all components into a cohesive user
experience.

### State Management

The application uses a simple state machine with two states:

- **IDLE**: Not recording, waiting for user input <br>
- **RECORDING**: Active transcription in progress <br>

The `state` attribute is reactive, meaning changes automatically trigger
UI updates through the `watch_state()` method.

### Recording State Indicators

**Visual feedback when recording:** - Header background turns red -
Title changes to “● RECORDING” - Footer shows recording-specific
shortcuts

**Visual feedback when idle:** - Header background normal - Title shows
“○ STANDBY (provider/model)” - Footer shows all available shortcuts

### AI Integration Strategy

The application maintains a single `TranscriptEditor` instance
throughout its lifetime. This is important because:

1.  **Memory Persistence**: The Chat inside TranscriptEditor maintains a
    `hist` (history) of all interactions
2.  **Cross-Session Context**: Even after stopping and restarting
    recording, the AI remembers previous context
3.  **Efficient Token Usage**: Conversation history allows contextual
    edits without re-sending entire transcript
4.  **Better Edit Detection**: The AI understands references like “that”
    or “the last part” from earlier in the session

### Transcription Flow

1.  **User presses SPACE** → `action_toggle_recording()` called
2.  **State changes to RECORDING** → UI updates via `watch_state()`
3.  **LiveTranscriber starts** → Begins capturing audio and running VAD
4.  **Whisper transcribes utterances** → Calls `on_transcript_chunk()`
    with each result
5.  **AI processes chunk** → `transcript_editor.process_chunk()`
    determines action
6.  **Display updates**:
    - For appends: Add new line to Log widget
    - For edits: Clear Log and redisplay full transcript
7.  **User presses SPACE again** → State returns to IDLE, transcriber
    stops

### Callback Pattern

The `on_transcript_chunk()` callback demonstrates graceful error
handling:

``` python
try:
    result = self.transcript_editor.process_chunk(text)
    # Update UI based on result
except Exception as e:
    # Fallback: just append text without AI processing
    self.transcript_display.write_line(text)
```

This ensures transcription continues even if AI processing fails
(network issues, API limits, etc.).

### Configuration Loading

On mount, the application: 1. Loads configuration from disk 2. Reads API
keys for all providers 3. Sets environment variables for LLM clients 4.
Checks if configuration is valid 5. Displays appropriate warnings if
setup is incomplete

### Keyboard Bindings

All keyboard shortcuts are declared in the `BINDINGS` class attribute:

- **q**: Quit application
- **?**: Open help modal
- **s**: Open audio settings
- **a**: Open AI model settings  
- **space**: Toggle recording on/off
- **c**: Copy transcript to clipboard

Textual automatically displays these in the footer and handles the key
events.
